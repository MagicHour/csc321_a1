{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "csc321_p1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MagicHour/csc321_a1/blob/master/csc321_p1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qdIR2lQMSYe",
        "colab_type": "text"
      },
      "source": [
        "# CSC321H5 Project 1. Music Millenium Classification\n",
        "\n",
        "**Deadline**: Thursday, Jan. 30, by 9pm\n",
        "\n",
        "**Submission**: Submit a PDF export of the completed notebook. \n",
        "\n",
        "**Late Submission**: Please see the syllabus for the late submission criteria.\n",
        "\n",
        "To celebrate the start of a new decade, we will build models to predict which\n",
        "**century** a piece of music was released.  We will be using the \"YearPredictionMSD Data Set\"\n",
        "based on the Million Song Dataset. The data is available to download from the UCI \n",
        "Machine Learning Repository. Here are some links about the data:\n",
        "\n",
        "- https://archive.ics.uci.edu/ml/datasets/yearpredictionmsd\n",
        "- http://millionsongdataset.com/pages/tasks-demos/#yearrecognition\n",
        "\n",
        "## Question 1. Data\n",
        "\n",
        "Start by setting up a Google Colab notebook in which to do your work.\n",
        "If you are working with a partner, you might find this link helpful:\n",
        "\n",
        "- https://colab.research.google.com/github/googlecolab/colabtools/blob/master/notebooks/colab-github-demo.ipynb\n",
        "\n",
        "The recommended way to work together is pair coding, where you and your partner are sitting together and writing code together."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MI7ZHNUkQL9f",
        "colab_type": "text"
      },
      "source": [
        "GREETINGS TRAVELLER\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AqYxuEHpMSYj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3MJ_QBXtMSYx",
        "colab_type": "text"
      },
      "source": [
        "Now that your notebook is set up, we can load the data into the notebook. The code below provides\n",
        "two ways of loading the data: directly from the internet, or through mounting Google Drive.\n",
        "The first method is easier but slower, and the second method is a bit involved at first, but\n",
        "can save you time later on. You will need to mount Google Drive for later assignments, so we recommend\n",
        "figuring how to do that now.\n",
        "\n",
        "Here are some resources to help you get started:\n",
        "\n",
        "- http://colab.research.google.com/notebooks/io.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OtRyzDe1MSY2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas\n",
        "load_from_drive = False\n",
        "\n",
        "if not load_from_drive:\n",
        "  csv_path = \"http://archive.ics.uci.edu/ml/machine-learning-databases/00203/YearPredictionMSD.txt.zip\"\n",
        "else:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/gdrive')\n",
        "  csv_path = '/content/drive/My Drive/csc321_a1/YearPredictionMSD.txt.zip' # TODO - UPDATE ME!\n",
        "\n",
        "t_label = [\"year\"]\n",
        "x_labels = [\"var%d\" % i for i in range(1, 91)]\n",
        "df = pandas.read_csv(csv_path, names=t_label + x_labels)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHTDqbZYMSY-",
        "colab_type": "text"
      },
      "source": [
        "Now that the data is loaded to your Colab notebook, you should be able to display the Pandas\n",
        "DataFrame `df` as a table:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zDKRDrSPMSZB",
        "colab_type": "code",
        "outputId": "b671b1eb-6068-4484-d82c-f5c82cf25a1d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        }
      },
      "source": [
        "df"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>year</th>\n",
              "      <th>var1</th>\n",
              "      <th>var2</th>\n",
              "      <th>var3</th>\n",
              "      <th>var4</th>\n",
              "      <th>var5</th>\n",
              "      <th>var6</th>\n",
              "      <th>var7</th>\n",
              "      <th>var8</th>\n",
              "      <th>var9</th>\n",
              "      <th>var10</th>\n",
              "      <th>var11</th>\n",
              "      <th>var12</th>\n",
              "      <th>var13</th>\n",
              "      <th>var14</th>\n",
              "      <th>var15</th>\n",
              "      <th>var16</th>\n",
              "      <th>var17</th>\n",
              "      <th>var18</th>\n",
              "      <th>var19</th>\n",
              "      <th>var20</th>\n",
              "      <th>var21</th>\n",
              "      <th>var22</th>\n",
              "      <th>var23</th>\n",
              "      <th>var24</th>\n",
              "      <th>var25</th>\n",
              "      <th>var26</th>\n",
              "      <th>var27</th>\n",
              "      <th>var28</th>\n",
              "      <th>var29</th>\n",
              "      <th>var30</th>\n",
              "      <th>var31</th>\n",
              "      <th>var32</th>\n",
              "      <th>var33</th>\n",
              "      <th>var34</th>\n",
              "      <th>var35</th>\n",
              "      <th>var36</th>\n",
              "      <th>var37</th>\n",
              "      <th>var38</th>\n",
              "      <th>var39</th>\n",
              "      <th>...</th>\n",
              "      <th>var51</th>\n",
              "      <th>var52</th>\n",
              "      <th>var53</th>\n",
              "      <th>var54</th>\n",
              "      <th>var55</th>\n",
              "      <th>var56</th>\n",
              "      <th>var57</th>\n",
              "      <th>var58</th>\n",
              "      <th>var59</th>\n",
              "      <th>var60</th>\n",
              "      <th>var61</th>\n",
              "      <th>var62</th>\n",
              "      <th>var63</th>\n",
              "      <th>var64</th>\n",
              "      <th>var65</th>\n",
              "      <th>var66</th>\n",
              "      <th>var67</th>\n",
              "      <th>var68</th>\n",
              "      <th>var69</th>\n",
              "      <th>var70</th>\n",
              "      <th>var71</th>\n",
              "      <th>var72</th>\n",
              "      <th>var73</th>\n",
              "      <th>var74</th>\n",
              "      <th>var75</th>\n",
              "      <th>var76</th>\n",
              "      <th>var77</th>\n",
              "      <th>var78</th>\n",
              "      <th>var79</th>\n",
              "      <th>var80</th>\n",
              "      <th>var81</th>\n",
              "      <th>var82</th>\n",
              "      <th>var83</th>\n",
              "      <th>var84</th>\n",
              "      <th>var85</th>\n",
              "      <th>var86</th>\n",
              "      <th>var87</th>\n",
              "      <th>var88</th>\n",
              "      <th>var89</th>\n",
              "      <th>var90</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2001</td>\n",
              "      <td>49.94357</td>\n",
              "      <td>21.47114</td>\n",
              "      <td>73.07750</td>\n",
              "      <td>8.74861</td>\n",
              "      <td>-17.40628</td>\n",
              "      <td>-13.09905</td>\n",
              "      <td>-25.01202</td>\n",
              "      <td>-12.23257</td>\n",
              "      <td>7.83089</td>\n",
              "      <td>-2.46783</td>\n",
              "      <td>3.32136</td>\n",
              "      <td>-2.31521</td>\n",
              "      <td>10.20556</td>\n",
              "      <td>611.10913</td>\n",
              "      <td>951.08960</td>\n",
              "      <td>698.11428</td>\n",
              "      <td>408.98485</td>\n",
              "      <td>383.70912</td>\n",
              "      <td>326.51512</td>\n",
              "      <td>238.11327</td>\n",
              "      <td>251.42414</td>\n",
              "      <td>187.17351</td>\n",
              "      <td>100.42652</td>\n",
              "      <td>179.19498</td>\n",
              "      <td>-8.41558</td>\n",
              "      <td>-317.87038</td>\n",
              "      <td>95.86266</td>\n",
              "      <td>48.10259</td>\n",
              "      <td>-95.66303</td>\n",
              "      <td>-18.06215</td>\n",
              "      <td>1.96984</td>\n",
              "      <td>34.42438</td>\n",
              "      <td>11.72670</td>\n",
              "      <td>1.36790</td>\n",
              "      <td>7.79444</td>\n",
              "      <td>-0.36994</td>\n",
              "      <td>-133.67852</td>\n",
              "      <td>-83.26165</td>\n",
              "      <td>-37.29765</td>\n",
              "      <td>...</td>\n",
              "      <td>-25.38187</td>\n",
              "      <td>-3.90772</td>\n",
              "      <td>13.29258</td>\n",
              "      <td>41.55060</td>\n",
              "      <td>-7.26272</td>\n",
              "      <td>-21.00863</td>\n",
              "      <td>105.50848</td>\n",
              "      <td>64.29856</td>\n",
              "      <td>26.08481</td>\n",
              "      <td>-44.59110</td>\n",
              "      <td>-8.30657</td>\n",
              "      <td>7.93706</td>\n",
              "      <td>-10.73660</td>\n",
              "      <td>-95.44766</td>\n",
              "      <td>-82.03307</td>\n",
              "      <td>-35.59194</td>\n",
              "      <td>4.69525</td>\n",
              "      <td>70.95626</td>\n",
              "      <td>28.09139</td>\n",
              "      <td>6.02015</td>\n",
              "      <td>-37.13767</td>\n",
              "      <td>-41.12450</td>\n",
              "      <td>-8.40816</td>\n",
              "      <td>7.19877</td>\n",
              "      <td>-8.60176</td>\n",
              "      <td>-5.90857</td>\n",
              "      <td>-12.32437</td>\n",
              "      <td>14.68734</td>\n",
              "      <td>-54.32125</td>\n",
              "      <td>40.14786</td>\n",
              "      <td>13.01620</td>\n",
              "      <td>-54.40548</td>\n",
              "      <td>58.99367</td>\n",
              "      <td>15.37344</td>\n",
              "      <td>1.11144</td>\n",
              "      <td>-23.08793</td>\n",
              "      <td>68.40795</td>\n",
              "      <td>-1.82223</td>\n",
              "      <td>-27.46348</td>\n",
              "      <td>2.26327</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2001</td>\n",
              "      <td>48.73215</td>\n",
              "      <td>18.42930</td>\n",
              "      <td>70.32679</td>\n",
              "      <td>12.94636</td>\n",
              "      <td>-10.32437</td>\n",
              "      <td>-24.83777</td>\n",
              "      <td>8.76630</td>\n",
              "      <td>-0.92019</td>\n",
              "      <td>18.76548</td>\n",
              "      <td>4.59210</td>\n",
              "      <td>2.21920</td>\n",
              "      <td>0.34006</td>\n",
              "      <td>44.38997</td>\n",
              "      <td>2056.93836</td>\n",
              "      <td>605.40696</td>\n",
              "      <td>457.41175</td>\n",
              "      <td>777.15347</td>\n",
              "      <td>415.64880</td>\n",
              "      <td>746.47775</td>\n",
              "      <td>366.45320</td>\n",
              "      <td>317.82946</td>\n",
              "      <td>273.07917</td>\n",
              "      <td>141.75921</td>\n",
              "      <td>317.35269</td>\n",
              "      <td>19.48271</td>\n",
              "      <td>-65.25496</td>\n",
              "      <td>162.75145</td>\n",
              "      <td>135.00765</td>\n",
              "      <td>-96.28436</td>\n",
              "      <td>-86.87955</td>\n",
              "      <td>17.38087</td>\n",
              "      <td>45.90742</td>\n",
              "      <td>32.49908</td>\n",
              "      <td>-32.85429</td>\n",
              "      <td>45.10830</td>\n",
              "      <td>26.84939</td>\n",
              "      <td>-302.57328</td>\n",
              "      <td>-41.71932</td>\n",
              "      <td>-138.85034</td>\n",
              "      <td>...</td>\n",
              "      <td>28.55107</td>\n",
              "      <td>1.52298</td>\n",
              "      <td>70.99515</td>\n",
              "      <td>-43.63073</td>\n",
              "      <td>-42.55014</td>\n",
              "      <td>129.82848</td>\n",
              "      <td>79.95420</td>\n",
              "      <td>-87.14554</td>\n",
              "      <td>-45.75446</td>\n",
              "      <td>-65.82100</td>\n",
              "      <td>-43.90031</td>\n",
              "      <td>-19.45705</td>\n",
              "      <td>12.59163</td>\n",
              "      <td>-407.64130</td>\n",
              "      <td>42.91189</td>\n",
              "      <td>12.15850</td>\n",
              "      <td>-88.37882</td>\n",
              "      <td>42.25246</td>\n",
              "      <td>46.49209</td>\n",
              "      <td>-30.17747</td>\n",
              "      <td>45.98495</td>\n",
              "      <td>130.47892</td>\n",
              "      <td>13.88281</td>\n",
              "      <td>-4.00055</td>\n",
              "      <td>17.85965</td>\n",
              "      <td>-18.32138</td>\n",
              "      <td>-87.99109</td>\n",
              "      <td>14.37524</td>\n",
              "      <td>-22.70119</td>\n",
              "      <td>-58.81266</td>\n",
              "      <td>5.66812</td>\n",
              "      <td>-19.68073</td>\n",
              "      <td>33.04964</td>\n",
              "      <td>42.87836</td>\n",
              "      <td>-9.90378</td>\n",
              "      <td>-32.22788</td>\n",
              "      <td>70.49388</td>\n",
              "      <td>12.04941</td>\n",
              "      <td>58.43453</td>\n",
              "      <td>26.92061</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2001</td>\n",
              "      <td>50.95714</td>\n",
              "      <td>31.85602</td>\n",
              "      <td>55.81851</td>\n",
              "      <td>13.41693</td>\n",
              "      <td>-6.57898</td>\n",
              "      <td>-18.54940</td>\n",
              "      <td>-3.27872</td>\n",
              "      <td>-2.35035</td>\n",
              "      <td>16.07017</td>\n",
              "      <td>1.39518</td>\n",
              "      <td>2.73553</td>\n",
              "      <td>0.82804</td>\n",
              "      <td>7.46586</td>\n",
              "      <td>699.54544</td>\n",
              "      <td>1016.00954</td>\n",
              "      <td>594.06748</td>\n",
              "      <td>355.73663</td>\n",
              "      <td>507.39931</td>\n",
              "      <td>387.69910</td>\n",
              "      <td>287.15347</td>\n",
              "      <td>112.37152</td>\n",
              "      <td>161.68928</td>\n",
              "      <td>144.14353</td>\n",
              "      <td>199.29693</td>\n",
              "      <td>-4.24359</td>\n",
              "      <td>-297.00587</td>\n",
              "      <td>-148.36392</td>\n",
              "      <td>-7.94726</td>\n",
              "      <td>-18.71630</td>\n",
              "      <td>12.77542</td>\n",
              "      <td>-25.37725</td>\n",
              "      <td>9.71410</td>\n",
              "      <td>0.13843</td>\n",
              "      <td>26.79723</td>\n",
              "      <td>6.30760</td>\n",
              "      <td>28.70107</td>\n",
              "      <td>-74.89005</td>\n",
              "      <td>-289.19553</td>\n",
              "      <td>-166.26089</td>\n",
              "      <td>...</td>\n",
              "      <td>18.50939</td>\n",
              "      <td>16.97216</td>\n",
              "      <td>24.26629</td>\n",
              "      <td>-10.50788</td>\n",
              "      <td>-8.68412</td>\n",
              "      <td>54.75759</td>\n",
              "      <td>194.74034</td>\n",
              "      <td>7.95966</td>\n",
              "      <td>-18.22685</td>\n",
              "      <td>0.06463</td>\n",
              "      <td>-2.63069</td>\n",
              "      <td>26.02561</td>\n",
              "      <td>1.75729</td>\n",
              "      <td>-262.36917</td>\n",
              "      <td>-233.60089</td>\n",
              "      <td>-2.50502</td>\n",
              "      <td>-12.14279</td>\n",
              "      <td>81.37617</td>\n",
              "      <td>2.07554</td>\n",
              "      <td>-1.82381</td>\n",
              "      <td>183.65292</td>\n",
              "      <td>22.64797</td>\n",
              "      <td>-39.98887</td>\n",
              "      <td>43.37381</td>\n",
              "      <td>-31.56737</td>\n",
              "      <td>-4.88840</td>\n",
              "      <td>-36.53213</td>\n",
              "      <td>-23.94662</td>\n",
              "      <td>-84.19275</td>\n",
              "      <td>66.00518</td>\n",
              "      <td>3.03800</td>\n",
              "      <td>26.05866</td>\n",
              "      <td>-50.92779</td>\n",
              "      <td>10.93792</td>\n",
              "      <td>-0.07568</td>\n",
              "      <td>43.20130</td>\n",
              "      <td>-115.00698</td>\n",
              "      <td>-0.05859</td>\n",
              "      <td>39.67068</td>\n",
              "      <td>-0.66345</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2001</td>\n",
              "      <td>48.24750</td>\n",
              "      <td>-1.89837</td>\n",
              "      <td>36.29772</td>\n",
              "      <td>2.58776</td>\n",
              "      <td>0.97170</td>\n",
              "      <td>-26.21683</td>\n",
              "      <td>5.05097</td>\n",
              "      <td>-10.34124</td>\n",
              "      <td>3.55005</td>\n",
              "      <td>-6.36304</td>\n",
              "      <td>6.63016</td>\n",
              "      <td>-3.35142</td>\n",
              "      <td>37.64085</td>\n",
              "      <td>2174.08189</td>\n",
              "      <td>697.43346</td>\n",
              "      <td>459.24587</td>\n",
              "      <td>742.78961</td>\n",
              "      <td>229.30783</td>\n",
              "      <td>387.89697</td>\n",
              "      <td>249.06662</td>\n",
              "      <td>245.89870</td>\n",
              "      <td>176.20527</td>\n",
              "      <td>98.82222</td>\n",
              "      <td>150.97286</td>\n",
              "      <td>78.49057</td>\n",
              "      <td>-62.00282</td>\n",
              "      <td>43.49659</td>\n",
              "      <td>-96.42719</td>\n",
              "      <td>-108.96608</td>\n",
              "      <td>14.22854</td>\n",
              "      <td>14.54178</td>\n",
              "      <td>-23.55608</td>\n",
              "      <td>-39.36953</td>\n",
              "      <td>-43.59209</td>\n",
              "      <td>20.83714</td>\n",
              "      <td>35.63919</td>\n",
              "      <td>-181.34947</td>\n",
              "      <td>-93.66614</td>\n",
              "      <td>-90.55616</td>\n",
              "      <td>...</td>\n",
              "      <td>4.56917</td>\n",
              "      <td>-37.32280</td>\n",
              "      <td>4.15159</td>\n",
              "      <td>12.24315</td>\n",
              "      <td>35.02697</td>\n",
              "      <td>-178.89573</td>\n",
              "      <td>82.46573</td>\n",
              "      <td>-20.49425</td>\n",
              "      <td>101.78577</td>\n",
              "      <td>-19.77808</td>\n",
              "      <td>-21.52657</td>\n",
              "      <td>3.36303</td>\n",
              "      <td>-11.63176</td>\n",
              "      <td>51.55411</td>\n",
              "      <td>-50.57576</td>\n",
              "      <td>-28.14755</td>\n",
              "      <td>-83.15795</td>\n",
              "      <td>-7.35260</td>\n",
              "      <td>-22.11505</td>\n",
              "      <td>1.18279</td>\n",
              "      <td>-122.70467</td>\n",
              "      <td>150.57360</td>\n",
              "      <td>24.37468</td>\n",
              "      <td>41.19821</td>\n",
              "      <td>-37.04318</td>\n",
              "      <td>-28.72986</td>\n",
              "      <td>162.19614</td>\n",
              "      <td>22.18309</td>\n",
              "      <td>-8.63509</td>\n",
              "      <td>85.23416</td>\n",
              "      <td>34.57337</td>\n",
              "      <td>-171.70734</td>\n",
              "      <td>-16.96705</td>\n",
              "      <td>-46.67617</td>\n",
              "      <td>-12.51516</td>\n",
              "      <td>82.58061</td>\n",
              "      <td>-72.08993</td>\n",
              "      <td>9.90558</td>\n",
              "      <td>199.62971</td>\n",
              "      <td>18.85382</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2001</td>\n",
              "      <td>50.97020</td>\n",
              "      <td>42.20998</td>\n",
              "      <td>67.09964</td>\n",
              "      <td>8.46791</td>\n",
              "      <td>-15.85279</td>\n",
              "      <td>-16.81409</td>\n",
              "      <td>-12.48207</td>\n",
              "      <td>-9.37636</td>\n",
              "      <td>12.63699</td>\n",
              "      <td>0.93609</td>\n",
              "      <td>1.60923</td>\n",
              "      <td>2.19223</td>\n",
              "      <td>47.32082</td>\n",
              "      <td>894.28471</td>\n",
              "      <td>809.86615</td>\n",
              "      <td>318.78559</td>\n",
              "      <td>435.04497</td>\n",
              "      <td>341.61467</td>\n",
              "      <td>334.30734</td>\n",
              "      <td>322.99589</td>\n",
              "      <td>190.61921</td>\n",
              "      <td>235.84715</td>\n",
              "      <td>96.89517</td>\n",
              "      <td>210.58870</td>\n",
              "      <td>5.60463</td>\n",
              "      <td>-199.63958</td>\n",
              "      <td>204.85812</td>\n",
              "      <td>-77.17695</td>\n",
              "      <td>-65.79741</td>\n",
              "      <td>-6.95097</td>\n",
              "      <td>-12.15262</td>\n",
              "      <td>-3.85410</td>\n",
              "      <td>20.68990</td>\n",
              "      <td>-20.30480</td>\n",
              "      <td>37.15045</td>\n",
              "      <td>11.20673</td>\n",
              "      <td>-124.09519</td>\n",
              "      <td>-295.98542</td>\n",
              "      <td>-33.31169</td>\n",
              "      <td>...</td>\n",
              "      <td>45.25506</td>\n",
              "      <td>10.42226</td>\n",
              "      <td>27.88782</td>\n",
              "      <td>-17.12676</td>\n",
              "      <td>-31.54772</td>\n",
              "      <td>-76.86293</td>\n",
              "      <td>41.17343</td>\n",
              "      <td>-138.32535</td>\n",
              "      <td>-53.96905</td>\n",
              "      <td>-21.30266</td>\n",
              "      <td>-24.87362</td>\n",
              "      <td>-2.46595</td>\n",
              "      <td>-4.05003</td>\n",
              "      <td>-56.51161</td>\n",
              "      <td>-34.56445</td>\n",
              "      <td>-5.07092</td>\n",
              "      <td>-47.75605</td>\n",
              "      <td>64.81513</td>\n",
              "      <td>-97.42948</td>\n",
              "      <td>-12.59418</td>\n",
              "      <td>55.23699</td>\n",
              "      <td>28.85657</td>\n",
              "      <td>54.53513</td>\n",
              "      <td>-31.97077</td>\n",
              "      <td>20.03279</td>\n",
              "      <td>-8.07892</td>\n",
              "      <td>-55.12617</td>\n",
              "      <td>26.58961</td>\n",
              "      <td>-10.27183</td>\n",
              "      <td>-30.64232</td>\n",
              "      <td>9.92661</td>\n",
              "      <td>-55.95724</td>\n",
              "      <td>64.92712</td>\n",
              "      <td>-17.72522</td>\n",
              "      <td>-1.49237</td>\n",
              "      <td>-7.50035</td>\n",
              "      <td>51.76631</td>\n",
              "      <td>7.88713</td>\n",
              "      <td>55.66926</td>\n",
              "      <td>28.74903</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>515340</th>\n",
              "      <td>2006</td>\n",
              "      <td>51.28467</td>\n",
              "      <td>45.88068</td>\n",
              "      <td>22.19582</td>\n",
              "      <td>-5.53319</td>\n",
              "      <td>-3.61835</td>\n",
              "      <td>-16.36914</td>\n",
              "      <td>2.12652</td>\n",
              "      <td>5.18160</td>\n",
              "      <td>-8.66890</td>\n",
              "      <td>2.67217</td>\n",
              "      <td>0.45234</td>\n",
              "      <td>2.51380</td>\n",
              "      <td>18.79583</td>\n",
              "      <td>592.17931</td>\n",
              "      <td>619.01842</td>\n",
              "      <td>681.30323</td>\n",
              "      <td>415.21939</td>\n",
              "      <td>639.90327</td>\n",
              "      <td>287.20710</td>\n",
              "      <td>375.31963</td>\n",
              "      <td>212.76265</td>\n",
              "      <td>246.26651</td>\n",
              "      <td>143.48234</td>\n",
              "      <td>217.45556</td>\n",
              "      <td>9.90577</td>\n",
              "      <td>-62.51153</td>\n",
              "      <td>-76.96635</td>\n",
              "      <td>-60.62065</td>\n",
              "      <td>67.81811</td>\n",
              "      <td>-9.20742</td>\n",
              "      <td>-30.73303</td>\n",
              "      <td>21.58525</td>\n",
              "      <td>-31.21664</td>\n",
              "      <td>-36.39659</td>\n",
              "      <td>28.18814</td>\n",
              "      <td>39.46981</td>\n",
              "      <td>-77.13200</td>\n",
              "      <td>-43.39948</td>\n",
              "      <td>-57.69462</td>\n",
              "      <td>...</td>\n",
              "      <td>-74.40960</td>\n",
              "      <td>78.78128</td>\n",
              "      <td>-14.74786</td>\n",
              "      <td>18.02148</td>\n",
              "      <td>-19.61304</td>\n",
              "      <td>-50.34714</td>\n",
              "      <td>87.06521</td>\n",
              "      <td>43.77874</td>\n",
              "      <td>-5.00339</td>\n",
              "      <td>101.08108</td>\n",
              "      <td>-13.34314</td>\n",
              "      <td>-59.17573</td>\n",
              "      <td>-46.22182</td>\n",
              "      <td>-27.10155</td>\n",
              "      <td>-7.07840</td>\n",
              "      <td>23.04732</td>\n",
              "      <td>29.32027</td>\n",
              "      <td>2.10740</td>\n",
              "      <td>-5.77951</td>\n",
              "      <td>2.68326</td>\n",
              "      <td>-13.78081</td>\n",
              "      <td>6.33542</td>\n",
              "      <td>-37.38191</td>\n",
              "      <td>-14.90918</td>\n",
              "      <td>26.87263</td>\n",
              "      <td>7.07232</td>\n",
              "      <td>-127.04955</td>\n",
              "      <td>86.78200</td>\n",
              "      <td>-68.14511</td>\n",
              "      <td>67.44416</td>\n",
              "      <td>4.81440</td>\n",
              "      <td>-3.75991</td>\n",
              "      <td>-30.92584</td>\n",
              "      <td>26.33968</td>\n",
              "      <td>-5.03390</td>\n",
              "      <td>21.86037</td>\n",
              "      <td>-142.29410</td>\n",
              "      <td>3.42901</td>\n",
              "      <td>-41.14721</td>\n",
              "      <td>-15.46052</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>515341</th>\n",
              "      <td>2006</td>\n",
              "      <td>49.87870</td>\n",
              "      <td>37.93125</td>\n",
              "      <td>18.65987</td>\n",
              "      <td>-3.63581</td>\n",
              "      <td>-27.75665</td>\n",
              "      <td>-18.52988</td>\n",
              "      <td>7.76108</td>\n",
              "      <td>3.56109</td>\n",
              "      <td>-2.50351</td>\n",
              "      <td>2.20175</td>\n",
              "      <td>-0.58487</td>\n",
              "      <td>-9.78657</td>\n",
              "      <td>35.81410</td>\n",
              "      <td>1047.28364</td>\n",
              "      <td>1451.87226</td>\n",
              "      <td>633.17982</td>\n",
              "      <td>448.46796</td>\n",
              "      <td>826.14418</td>\n",
              "      <td>277.55902</td>\n",
              "      <td>202.20787</td>\n",
              "      <td>241.85866</td>\n",
              "      <td>199.31274</td>\n",
              "      <td>180.60934</td>\n",
              "      <td>168.49980</td>\n",
              "      <td>89.28058</td>\n",
              "      <td>237.30605</td>\n",
              "      <td>-72.22211</td>\n",
              "      <td>-10.02772</td>\n",
              "      <td>-41.24980</td>\n",
              "      <td>-7.59473</td>\n",
              "      <td>-5.23307</td>\n",
              "      <td>24.88978</td>\n",
              "      <td>39.42813</td>\n",
              "      <td>-40.17760</td>\n",
              "      <td>26.51372</td>\n",
              "      <td>79.84191</td>\n",
              "      <td>-15.49724</td>\n",
              "      <td>46.37942</td>\n",
              "      <td>-209.97900</td>\n",
              "      <td>...</td>\n",
              "      <td>-61.06002</td>\n",
              "      <td>50.86072</td>\n",
              "      <td>-3.54799</td>\n",
              "      <td>36.50303</td>\n",
              "      <td>20.94570</td>\n",
              "      <td>-79.43478</td>\n",
              "      <td>-15.49133</td>\n",
              "      <td>17.79165</td>\n",
              "      <td>95.84510</td>\n",
              "      <td>-37.68620</td>\n",
              "      <td>8.51302</td>\n",
              "      <td>13.72492</td>\n",
              "      <td>-71.83419</td>\n",
              "      <td>-191.37407</td>\n",
              "      <td>-34.71662</td>\n",
              "      <td>28.34789</td>\n",
              "      <td>45.25187</td>\n",
              "      <td>17.07862</td>\n",
              "      <td>31.46894</td>\n",
              "      <td>-13.44802</td>\n",
              "      <td>38.68815</td>\n",
              "      <td>109.03046</td>\n",
              "      <td>-42.45525</td>\n",
              "      <td>18.67531</td>\n",
              "      <td>-50.86612</td>\n",
              "      <td>11.26242</td>\n",
              "      <td>59.30165</td>\n",
              "      <td>178.15846</td>\n",
              "      <td>-29.04997</td>\n",
              "      <td>70.22336</td>\n",
              "      <td>32.38589</td>\n",
              "      <td>-32.75535</td>\n",
              "      <td>-61.05473</td>\n",
              "      <td>56.65182</td>\n",
              "      <td>15.29965</td>\n",
              "      <td>95.88193</td>\n",
              "      <td>-10.63242</td>\n",
              "      <td>12.96552</td>\n",
              "      <td>92.11633</td>\n",
              "      <td>10.88815</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>515342</th>\n",
              "      <td>2006</td>\n",
              "      <td>45.12852</td>\n",
              "      <td>12.65758</td>\n",
              "      <td>-38.72018</td>\n",
              "      <td>8.80882</td>\n",
              "      <td>-29.29985</td>\n",
              "      <td>-2.28706</td>\n",
              "      <td>-18.40424</td>\n",
              "      <td>-22.28726</td>\n",
              "      <td>-4.52429</td>\n",
              "      <td>-11.46411</td>\n",
              "      <td>3.28514</td>\n",
              "      <td>1.99943</td>\n",
              "      <td>27.77109</td>\n",
              "      <td>1693.72442</td>\n",
              "      <td>3825.48305</td>\n",
              "      <td>2714.53243</td>\n",
              "      <td>1036.34216</td>\n",
              "      <td>1171.81248</td>\n",
              "      <td>468.44308</td>\n",
              "      <td>1042.15436</td>\n",
              "      <td>278.94429</td>\n",
              "      <td>497.83085</td>\n",
              "      <td>423.82729</td>\n",
              "      <td>239.91028</td>\n",
              "      <td>-61.01287</td>\n",
              "      <td>-1383.48696</td>\n",
              "      <td>-1828.43740</td>\n",
              "      <td>-131.54731</td>\n",
              "      <td>138.81510</td>\n",
              "      <td>51.36991</td>\n",
              "      <td>-45.25035</td>\n",
              "      <td>138.31791</td>\n",
              "      <td>-107.60348</td>\n",
              "      <td>-17.01878</td>\n",
              "      <td>-36.53276</td>\n",
              "      <td>226.67213</td>\n",
              "      <td>716.76768</td>\n",
              "      <td>-267.06525</td>\n",
              "      <td>-362.27860</td>\n",
              "      <td>...</td>\n",
              "      <td>191.56779</td>\n",
              "      <td>72.49396</td>\n",
              "      <td>-38.96949</td>\n",
              "      <td>61.22195</td>\n",
              "      <td>24.49062</td>\n",
              "      <td>182.62433</td>\n",
              "      <td>510.41684</td>\n",
              "      <td>-379.38804</td>\n",
              "      <td>226.54992</td>\n",
              "      <td>-201.28237</td>\n",
              "      <td>6.89971</td>\n",
              "      <td>86.07237</td>\n",
              "      <td>-42.85773</td>\n",
              "      <td>-215.01900</td>\n",
              "      <td>88.60866</td>\n",
              "      <td>14.51385</td>\n",
              "      <td>-28.33832</td>\n",
              "      <td>255.17385</td>\n",
              "      <td>14.17125</td>\n",
              "      <td>25.06417</td>\n",
              "      <td>218.85618</td>\n",
              "      <td>-222.53173</td>\n",
              "      <td>35.58546</td>\n",
              "      <td>30.88622</td>\n",
              "      <td>-24.91594</td>\n",
              "      <td>-2.65009</td>\n",
              "      <td>-69.53483</td>\n",
              "      <td>333.67598</td>\n",
              "      <td>-28.24399</td>\n",
              "      <td>202.51566</td>\n",
              "      <td>-18.73598</td>\n",
              "      <td>-71.15954</td>\n",
              "      <td>-123.98443</td>\n",
              "      <td>121.26989</td>\n",
              "      <td>10.89629</td>\n",
              "      <td>34.62409</td>\n",
              "      <td>-248.61020</td>\n",
              "      <td>-6.07171</td>\n",
              "      <td>53.96319</td>\n",
              "      <td>-8.09364</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>515343</th>\n",
              "      <td>2006</td>\n",
              "      <td>44.16614</td>\n",
              "      <td>32.38368</td>\n",
              "      <td>-3.34971</td>\n",
              "      <td>-2.49165</td>\n",
              "      <td>-19.59278</td>\n",
              "      <td>-18.67098</td>\n",
              "      <td>8.78428</td>\n",
              "      <td>4.02039</td>\n",
              "      <td>-12.01230</td>\n",
              "      <td>-0.74075</td>\n",
              "      <td>-1.26523</td>\n",
              "      <td>-4.41983</td>\n",
              "      <td>140.44937</td>\n",
              "      <td>2850.23336</td>\n",
              "      <td>1875.28895</td>\n",
              "      <td>1362.98053</td>\n",
              "      <td>784.39737</td>\n",
              "      <td>908.09838</td>\n",
              "      <td>367.12005</td>\n",
              "      <td>692.58547</td>\n",
              "      <td>286.72625</td>\n",
              "      <td>395.46735</td>\n",
              "      <td>221.19089</td>\n",
              "      <td>211.62098</td>\n",
              "      <td>141.17304</td>\n",
              "      <td>647.52054</td>\n",
              "      <td>-451.67671</td>\n",
              "      <td>-170.33993</td>\n",
              "      <td>-106.30851</td>\n",
              "      <td>129.80285</td>\n",
              "      <td>-118.54997</td>\n",
              "      <td>116.14019</td>\n",
              "      <td>-18.36186</td>\n",
              "      <td>-29.42843</td>\n",
              "      <td>13.59803</td>\n",
              "      <td>296.86552</td>\n",
              "      <td>-332.24640</td>\n",
              "      <td>219.84847</td>\n",
              "      <td>-180.27193</td>\n",
              "      <td>...</td>\n",
              "      <td>14.33401</td>\n",
              "      <td>-10.61959</td>\n",
              "      <td>-37.44137</td>\n",
              "      <td>32.72492</td>\n",
              "      <td>-16.62357</td>\n",
              "      <td>-343.07974</td>\n",
              "      <td>148.00075</td>\n",
              "      <td>-64.73672</td>\n",
              "      <td>59.16029</td>\n",
              "      <td>-129.60142</td>\n",
              "      <td>24.47146</td>\n",
              "      <td>-90.78617</td>\n",
              "      <td>-34.58624</td>\n",
              "      <td>-285.37506</td>\n",
              "      <td>-8.78066</td>\n",
              "      <td>63.91160</td>\n",
              "      <td>58.86067</td>\n",
              "      <td>43.28537</td>\n",
              "      <td>22.69472</td>\n",
              "      <td>-0.93940</td>\n",
              "      <td>417.76862</td>\n",
              "      <td>78.26912</td>\n",
              "      <td>-173.95232</td>\n",
              "      <td>-35.42845</td>\n",
              "      <td>10.59859</td>\n",
              "      <td>-16.51518</td>\n",
              "      <td>157.75671</td>\n",
              "      <td>294.31838</td>\n",
              "      <td>-37.30155</td>\n",
              "      <td>80.00327</td>\n",
              "      <td>67.16763</td>\n",
              "      <td>282.77624</td>\n",
              "      <td>-4.63677</td>\n",
              "      <td>144.00125</td>\n",
              "      <td>21.62652</td>\n",
              "      <td>-29.72432</td>\n",
              "      <td>71.47198</td>\n",
              "      <td>20.32240</td>\n",
              "      <td>14.83107</td>\n",
              "      <td>39.74909</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>515344</th>\n",
              "      <td>2005</td>\n",
              "      <td>51.85726</td>\n",
              "      <td>59.11655</td>\n",
              "      <td>26.39436</td>\n",
              "      <td>-5.46030</td>\n",
              "      <td>-20.69012</td>\n",
              "      <td>-19.95528</td>\n",
              "      <td>-6.72771</td>\n",
              "      <td>2.29590</td>\n",
              "      <td>10.31018</td>\n",
              "      <td>6.26597</td>\n",
              "      <td>-1.78800</td>\n",
              "      <td>-6.19786</td>\n",
              "      <td>20.16600</td>\n",
              "      <td>598.45275</td>\n",
              "      <td>1140.69539</td>\n",
              "      <td>721.49244</td>\n",
              "      <td>272.84841</td>\n",
              "      <td>564.06690</td>\n",
              "      <td>199.41547</td>\n",
              "      <td>189.04637</td>\n",
              "      <td>217.32042</td>\n",
              "      <td>137.13390</td>\n",
              "      <td>150.34608</td>\n",
              "      <td>98.21589</td>\n",
              "      <td>48.12644</td>\n",
              "      <td>-601.59295</td>\n",
              "      <td>10.58466</td>\n",
              "      <td>-83.35368</td>\n",
              "      <td>96.86756</td>\n",
              "      <td>69.40708</td>\n",
              "      <td>8.06033</td>\n",
              "      <td>-26.01693</td>\n",
              "      <td>-2.93173</td>\n",
              "      <td>26.18398</td>\n",
              "      <td>-12.24660</td>\n",
              "      <td>-14.52391</td>\n",
              "      <td>-121.61676</td>\n",
              "      <td>119.15632</td>\n",
              "      <td>-229.55722</td>\n",
              "      <td>...</td>\n",
              "      <td>1.78072</td>\n",
              "      <td>64.75548</td>\n",
              "      <td>24.55866</td>\n",
              "      <td>-1.12509</td>\n",
              "      <td>-13.58287</td>\n",
              "      <td>-99.66038</td>\n",
              "      <td>-124.73875</td>\n",
              "      <td>67.02630</td>\n",
              "      <td>33.05618</td>\n",
              "      <td>60.25818</td>\n",
              "      <td>28.00288</td>\n",
              "      <td>10.62425</td>\n",
              "      <td>-8.86772</td>\n",
              "      <td>78.13543</td>\n",
              "      <td>-181.10013</td>\n",
              "      <td>74.69489</td>\n",
              "      <td>57.45083</td>\n",
              "      <td>114.08816</td>\n",
              "      <td>-9.91322</td>\n",
              "      <td>7.53612</td>\n",
              "      <td>97.06395</td>\n",
              "      <td>233.17754</td>\n",
              "      <td>-100.68441</td>\n",
              "      <td>27.67012</td>\n",
              "      <td>-37.33008</td>\n",
              "      <td>-0.34676</td>\n",
              "      <td>-207.78766</td>\n",
              "      <td>116.75005</td>\n",
              "      <td>-91.82912</td>\n",
              "      <td>8.35020</td>\n",
              "      <td>-11.50511</td>\n",
              "      <td>-69.18291</td>\n",
              "      <td>60.58456</td>\n",
              "      <td>28.64599</td>\n",
              "      <td>-4.39620</td>\n",
              "      <td>-64.56491</td>\n",
              "      <td>-45.61012</td>\n",
              "      <td>-5.51512</td>\n",
              "      <td>32.35602</td>\n",
              "      <td>12.17352</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>515345 rows × 91 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        year      var1      var2  ...     var88      var89     var90\n",
              "0       2001  49.94357  21.47114  ...  -1.82223  -27.46348   2.26327\n",
              "1       2001  48.73215  18.42930  ...  12.04941   58.43453  26.92061\n",
              "2       2001  50.95714  31.85602  ...  -0.05859   39.67068  -0.66345\n",
              "3       2001  48.24750  -1.89837  ...   9.90558  199.62971  18.85382\n",
              "4       2001  50.97020  42.20998  ...   7.88713   55.66926  28.74903\n",
              "...      ...       ...       ...  ...       ...        ...       ...\n",
              "515340  2006  51.28467  45.88068  ...   3.42901  -41.14721 -15.46052\n",
              "515341  2006  49.87870  37.93125  ...  12.96552   92.11633  10.88815\n",
              "515342  2006  45.12852  12.65758  ...  -6.07171   53.96319  -8.09364\n",
              "515343  2006  44.16614  32.38368  ...  20.32240   14.83107  39.74909\n",
              "515344  2005  51.85726  59.11655  ...  -5.51512   32.35602  12.17352\n",
              "\n",
              "[515345 rows x 91 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekukgvbkMSZL",
        "colab_type": "text"
      },
      "source": [
        "To set up our data for classification, we'll use the \"year\" field to represent\n",
        "whether a song was released in the 20-th century. In our case `df[\"year\"]` will be 1 if\n",
        "the year was released after 2000, and 0 otherwise."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-wVr5lWMSZO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df[\"year\"] = df[\"year\"].map(lambda x: int(x > 2000))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mq4JNiTsMSZa",
        "colab_type": "text"
      },
      "source": [
        "### Part (a) -- 2 pts\n",
        "\n",
        "The data set description text asks us to respect the below train/test split to\n",
        "avoid the \"producer effect\". That is, we want to make sure that no song from a single artist\n",
        "ends up in both the training and test set.\n",
        "\n",
        "Explain why it would be problematic to have\n",
        "some songs from an artist in the training set, and other songs from the same artist in the\n",
        "test set. (Hint: Remember that we want our test accuracy to predict how well the model\n",
        "will perform in practice on a song it hasn't learned about.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vIGxn55CMSZc",
        "colab_type": "code",
        "outputId": "ca95bb48-c5bf-4069-e189-fb4a775205d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "df_train = df[:463715]\n",
        "df_test = df[463715:]\n",
        "\n",
        "# conver to numpy\n",
        "train_xs = df_train[x_labels].to_numpy()\n",
        "train_ts = df_train[t_label].to_numpy()\n",
        "test_xs = df_test[x_labels].to_numpy()\n",
        "test_ts = df_test[t_label].to_numpy()\n",
        "\n",
        "# Write your explanation here\n",
        "'''\n",
        "This is because training based on a song from an artists also trains the model\n",
        "for other songs from the same artist. This is because songs from the same artist\n",
        "can be similar, so using the test data will essentially contain part of our\n",
        "training data. Which would be problematic since we already learned from these\n",
        "songs from the same artist.\n",
        "'''"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nThis is because training based on a song from an artists also trains the model\\nfor other songs from the same artist. This is because songs from the same artist\\ncan be similar, so using the test data will essentially contain part of our\\ntraining data. Which would be problematic since we already learned from these\\nsongs from the same artist.\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nzsW4nmNMSZj",
        "colab_type": "text"
      },
      "source": [
        "### Part (b) -- 1 pts\n",
        "\n",
        "It can be beneficial to **normalize** the columns, so that each column (feature)\n",
        "has the *same* mean and standard deviation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3uST_Y7rMSZm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "feature_means = df_train.mean()[1:].to_numpy() # the [1:] removes the mean of the \"year\" field\n",
        "feature_stds  = df_train.std()[1:].to_numpy()\n",
        "\n",
        "train_norm_xs = (train_xs - feature_means) / feature_stds\n",
        "test_norm_xs = (test_xs - feature_means) / feature_stds\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27i-DvcyMSZv",
        "colab_type": "text"
      },
      "source": [
        "Notice how in our code, we normalized the test set using the *training data means and standard deviations*.\n",
        "This is *not* a bug.\n",
        "\n",
        "Explain why it would be improper to compute and use test set means\n",
        "and standard deviations. (Hint: Remember what we want to use the test accuracy to measure.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ce-xTT21MSZx",
        "colab_type": "code",
        "outputId": "a0c94beb-18e6-49a3-f06b-a1582f0540b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "# Write your explanation here\n",
        "\n",
        "'''\n",
        "If we use the test set means and standard deviations we are placing data within\n",
        "the specified ranges of the test data, which maybe different from the training\n",
        "data. If this is the case the model can be inaccurate as it is only trained for\n",
        "data within the specified ranges from the training data.\n",
        "'''"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nIf we use the test set means and standard deviations we are placing data within\\nthe specified ranges of the test data, which maybe different from the training\\ndata. If this is the case the model can be inaccurate as it is only trained for\\ndata within the specified ranges from the training data.\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hk0wqk8EMSZ3",
        "colab_type": "text"
      },
      "source": [
        "### Part (c) -- 1 pts\n",
        "\n",
        "Finally, we'll move some of the data in our training set into a validation set.\n",
        "\n",
        "Explain why we should limit how many times we use the test set, and that we should use the validation\n",
        "set during the model building process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PlOp8skBMSZ5",
        "colab_type": "code",
        "outputId": "619e7e27-ddfc-4ece-c9d8-87272c4fe4c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "# shuffle the training set\n",
        "reindex = np.random.permutation(len(train_xs))\n",
        "train_xs = train_xs[reindex]\n",
        "train_norm_xs = train_norm_xs[reindex]\n",
        "train_ts = train_ts[reindex]\n",
        "\n",
        "# use the first 50000 elements of `train_xs` as the validation set\n",
        "train_xs, val_xs           = train_xs[50000:], train_xs[:50000]\n",
        "train_norm_xs, val_norm_xs = train_norm_xs[50000:], train_norm_xs[:50000]\n",
        "train_ts, val_ts           = train_ts[50000:], train_ts[:50000]\n",
        "\n",
        "# Write your explanation here\n",
        "\n",
        "'''\n",
        "We should use the validation set during the model building process and limit\n",
        "the usage of the test set because, hyperparameters will change depending on the\n",
        "validation set. Whereas, the test set should be the unknown, that the model\n",
        "has not seen yet. By using the test model we are essentially breaking this\n",
        "rule that the test data should be unknown. Therefore, the test data should be\n",
        "limited in use to keep it unknown.\n",
        "'''\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nWe should use the validation set during the model building process and limit\\nthe usage of the test set because, hyperparameters will change depending on the\\nvalidation set. Whereas, the test set should be the unknown, that the model\\nhas not seen yet. By using the test model we are essentially breaking this\\nrule that the test data should be unknown. Therefore, the test data should be\\nlimited in use to keep it unknown.\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gs2MYChcMSaA",
        "colab_type": "text"
      },
      "source": [
        "## Part 2. Classification\n",
        "\n",
        "We will first build a *classification* model to perform decade classification.\n",
        "These helper functions are written for you. All other code that you write in this\n",
        "section should be vectorized whenever possible, and you will be penalized for \n",
        "not vectorizing your code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5AhrZXSzMSaD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sigmoid(z):\n",
        "  return 1 / (1 + np.exp(-z))\n",
        "    \n",
        "def cross_entropy(t, y):\n",
        "  return -t * np.log(y) - (1 - t) * np.log(1 - y)\n",
        "\n",
        "def cost(y, t):\n",
        "  return np.mean(cross_entropy(t, y))\n",
        "\n",
        "def get_accuracy(y, t):\n",
        "  acc = 0\n",
        "  N = 0\n",
        "  for i in range(len(y)):\n",
        "    N += 1\n",
        "    if (y[i] >= 0.5 and t[i] == 1) or (y[i] < 0.5 and t[i] == 0):\n",
        "      acc += 1\n",
        "  return acc / N"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0MnImhFZMSaL",
        "colab_type": "text"
      },
      "source": [
        "### Part (a) -- 2 pts\n",
        "\n",
        "Write a function `pred` that computes the prediction `y` based on weights `w` and bias `b`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p07MeOITMSaN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pred(w, b, X):\n",
        "  \"\"\"\n",
        "  Returns the prediction `y` of the target based on the weights `w` and scalar bias `b`.\n",
        "\n",
        "  Preconditions: np.shape(w) == (90,)\n",
        "                 type(b) == float\n",
        "                 np.shape(X) = (N, 90) for some N\n",
        "\n",
        "  >>> pred(np.zeros(90), 1, np.ones([2, 90]))\n",
        "  array([0.73105858, 0.73105858]) # It's okay if your output differs in the last decimals\n",
        "  \"\"\"\n",
        "  # Your code goes here\n",
        "  w = w.reshape(len(w), 1)\n",
        "  z = np.matmul(X, w) + b\n",
        "  out = sigmoid(z)\n",
        "  return out.flatten()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oyK4O8FfMSaU",
        "colab_type": "text"
      },
      "source": [
        "### Part (b) -- 3 pts\n",
        "\n",
        "Write a function `derivative_cost` that computes and returns the gradients \n",
        "$\\frac{\\partial\\mathcal{E}}{\\partial {\\bf w}}$ and\n",
        "$\\frac{\\partial\\mathcal{E}}{\\partial b}$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QMPJCwavMSaW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def derivative_cost(X, y, t):\n",
        "  \"\"\"\n",
        "  Returns a tuple containing the gradients dEdw and dEdb.\n",
        "\n",
        "  Precondition: np.shape(X) == (N, 90) for some N\n",
        "                np.shape(y) == (N,)\n",
        "                np.shape(t) == (N,)\n",
        "\n",
        "  Postcondition: np.shape(dEdw) = (90,)\n",
        "           type(dEdb) = float\n",
        "  \"\"\"\n",
        "  # Your code goes here\n",
        "  # return np.zeros(90), 0.\n",
        "  N = len(y)\n",
        "  y = y.reshape(len(y), 1)\n",
        "  t = t.reshape(len(t), 1)\n",
        "  y_t = y-t\n",
        "  \n",
        "  dEdw = (1/N)*(np.matmul(X.T, y_t))\n",
        "  dEdb = (1/N)*np.sum(y_t)\n",
        "  dEdw = dEdw.flatten()\n",
        "  return (dEdw, dEdb)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOn06-JWMSaf",
        "colab_type": "text"
      },
      "source": [
        "### Part (c) -- 2 pts\n",
        "\n",
        "We can check that our derivative is implemented correctly using the finite difference rule. In 1D, the\n",
        "finite difference rule tells us that for small $h$, we should have\n",
        "\n",
        "$$\\frac{f(x+h) - f(x)}{h} \\approx f'(x)$$\n",
        "\n",
        "Prove to yourself (and your TA) that $\\frac{\\partial\\mathcal{E}}{\\partial b}$  is implement correctly\n",
        "by comparing the result from `derivative_cost` with the value of `(pred(w, b + h, X) - pred(w, b, X)) / h`.\n",
        "Justify your choice of `w`, `b`, and `X`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zzp6l5nDMSah",
        "colab_type": "code",
        "outputId": "5b63edf8-271e-4f27-9de5-1e10e18c8da1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "# Your code goes here\n",
        "#(pred(w, b + h, X) - pred(w, b, X)) / h\n",
        "'''\n",
        "w = np.zeros(90)\n",
        "b = -91\n",
        "h = 0.000001\n",
        "X = np.zeros([2, 90])\n",
        "expected = np.e/((1+np.e)**2)\n",
        "y = pred(w, b, X)\n",
        "t = pred(w, b, X)\n",
        "val1 = (pred(w, b + h, X) - pred(w, b, X)) / h\n",
        "dEdw, dEdb = derivative_cost(X, y, y)\n",
        "print(val1)\n",
        "'''\n",
        "#print(w)\n",
        "#print(calculated_w)\n",
        "#print(np.sum(w-calculated_w))\n",
        "#print(calculated_w.shape)\n",
        "#print((dEdw, dEdb))\n",
        "#TODO"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nw = np.zeros(90)\\nb = -91\\nh = 0.000001\\nX = np.zeros([2, 90])\\nexpected = np.e/((1+np.e)**2)\\ny = pred(w, b, X)\\nt = pred(w, b, X)\\nval1 = (pred(w, b + h, X) - pred(w, b, X)) / h\\ndEdw, dEdb = derivative_cost(X, y, y)\\nprint(val1)\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0KY41xwzMSam",
        "colab_type": "text"
      },
      "source": [
        "### Part (d) -- 2 pts\n",
        "\n",
        "Prove to yourself (and your TA) that $\\frac{\\partial\\mathcal{E}}{\\partial {\\bf w}}$  is implement correctly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y7v2lDuuMSao",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Your code goes here. You might find this below code helpful: but it's\n",
        "# up to you to figure out how/why, and how to modify the code\n",
        "h = 0.000001\n",
        "H = np.zeros(90)\n",
        "H[0] = h\n",
        "#TODO"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VBFGwPU7MSau",
        "colab_type": "text"
      },
      "source": [
        "### Part (e) -- 4 pts\n",
        "\n",
        "Now that you have a gradient function that works, we can actually run gradient descent! Complete\n",
        "the following code that will run stochastic: gradient descent training:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwkvi19oMSaw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def run_gradient_descent(w0, b0, alpha=0.1, batch_size=100, max_iters=100):\n",
        "  \"\"\"Return the values of (w, b) after running gradient descent for max_iters.\n",
        "  We use:\n",
        "    - train_norm_xs and train_ts as the training set\n",
        "    - val_norm_xs and val_ts as the test set\n",
        "    - alpha as the learning rate\n",
        "    - (w0, b0) as the initial values of (w, b)\n",
        "\n",
        "  Precondition: np.shape(w0) == (90,)\n",
        "                type(b0) == float\n",
        " \n",
        "  Postcondition: np.shape(w) == (90,)\n",
        "                 type(b) == float\n",
        "  \"\"\"\n",
        "  global train_xs\n",
        "  global train_norm_xs\n",
        "  global train_ts\n",
        "  train_xs_c = np.copy(train_xs)\n",
        "  train_norm_xs_c = np.copy(train_norm_xs)\n",
        "  train_ts_c = np.copy(train_ts)\n",
        "\n",
        "  w = w0\n",
        "  b = b0\n",
        "  iter = 0\n",
        "\n",
        "  while iter < max_iters:\n",
        "    # shuffle the training set (there is code above for how to do this)\n",
        "    reindex = np.random.permutation(len(train_xs_c))\n",
        "    train_xs_c = train_xs_c[reindex]\n",
        "    train_norm_xs_c = train_norm_xs_c[reindex]\n",
        "    train_ts_c = train_ts_c[reindex]\n",
        "    \n",
        "    for i in range(0, len(train_norm_xs_c), batch_size): # iterate over each minibatch\n",
        "      # minibatch that we are working with:\n",
        "      X = train_norm_xs_c[i:(i + batch_size)]\n",
        "      t = train_ts_c[i:(i + batch_size), 0]\n",
        "\n",
        "      # since len(train_norm_xs) does not divide batch_size evenly, we will skip over\n",
        "      # the \"last\" minibatch\n",
        "      if np.shape(X)[0] != batch_size:\n",
        "        continue\n",
        "\n",
        "      # compute the prediction\n",
        "      # y = ...\n",
        "      y = pred(w, b, X)\n",
        "\n",
        "      # update w and b\n",
        "      # dw, db = ...\n",
        "      # w = ...\n",
        "      # b = ...\n",
        "      dw, db = derivative_cost(X, y, t)\n",
        "      w = w -alpha*dw\n",
        "      b = b-alpha*db\n",
        "\n",
        "      # increment the iteration count\n",
        "      iter += 1\n",
        "\n",
        "      # compute and plot the *validation* loss and accuracy\n",
        "      #if (iter % 10 == 0):\n",
        "        #train_cost = ...\n",
        "        #val_y = ...\n",
        "        #val_cost = ...\n",
        "        #val_acc = ...\n",
        "        #print(\"Iter %d. [Val Acc %.0f%%, Loss %f] [Train Loss %f]\" % (\n",
        "        #        iter, val_acc * 100, val_cost, train_cost))\n",
        "      if (iter % 10 == 0):\n",
        "        #The cost() function does not seem to work with 50,000 values\n",
        "        #and it runs out of memory\n",
        "        #as a result we will be using the first 1,000 validation values\n",
        "        train_cost = cost(y,t)\n",
        "        val_y = pred(w, b, val_norm_xs)\n",
        "        val_cost = cost(pred(w,b,val_norm_xs[:1000]), val_ts[:1000])\n",
        "        val_acc = get_accuracy(val_y[:1000],val_ts[:1000])\n",
        "        print(\"Iter %d. [Val Acc %.0f%%, Loss %f] [Train Loss %f]\" % (\n",
        "                iter, val_acc * 100, val_cost, train_cost))\n",
        "\n",
        "      if iter >= max_iters:\n",
        "        break\n",
        "  \n",
        "  return (w, b)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHp_G8paMSa1",
        "colab_type": "text"
      },
      "source": [
        "### Part (f) -- 2 pts\n",
        "\n",
        "Call `run_gradient_descent` with the weights and biases all initialized to zero.\n",
        "Show that if `alpha` is too small, then convergence is slow.\n",
        "Also, show that if `alpha` is too large, then we do not converge at all!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CKudlx3HMSa3",
        "colab_type": "code",
        "outputId": "ff0cf7a8-8296-4eba-8724-0c1e543b1b7b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 844
        }
      },
      "source": [
        "w0 = np.zeros(90)\n",
        "b0 = 0.\n",
        "# Write your code here\n",
        "print('---Run with alpha = 0.001---')\n",
        "run_gradient_descent(w0,b0,alpha=0.001)\n",
        "\n",
        "#Notice the validation accuracy does not change\n",
        "\n",
        "print('---Run with alpha = 10---') \n",
        "run_gradient_descent(w0,b0,alpha=1)\n",
        "\n",
        "#Notice the validation accuracies jump from place to place and does not converge.\n"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---Run with alpha = 0.001---\n",
            "Iter 10. [Val Acc 64%, Loss 0.693091] [Train Loss 0.692715]\n",
            "Iter 20. [Val Acc 65%, Loss 0.693049] [Train Loss 0.691297]\n",
            "Iter 30. [Val Acc 67%, Loss 0.693017] [Train Loss 0.691791]\n",
            "Iter 40. [Val Acc 65%, Loss 0.692991] [Train Loss 0.690540]\n",
            "Iter 50. [Val Acc 65%, Loss 0.692971] [Train Loss 0.687667]\n",
            "Iter 60. [Val Acc 65%, Loss 0.692935] [Train Loss 0.688155]\n",
            "Iter 70. [Val Acc 66%, Loss 0.692926] [Train Loss 0.688793]\n",
            "Iter 80. [Val Acc 66%, Loss 0.692922] [Train Loss 0.687485]\n",
            "Iter 90. [Val Acc 66%, Loss 0.692924] [Train Loss 0.686157]\n",
            "Iter 100. [Val Acc 66%, Loss 0.692943] [Train Loss 0.688716]\n",
            "---Run with alpha = 10---\n",
            "Iter 10. [Val Acc 64%, Loss 0.858758] [Train Loss 0.637745]\n",
            "Iter 20. [Val Acc 69%, Loss 0.852327] [Train Loss 0.652451]\n",
            "Iter 30. [Val Acc 71%, Loss 0.893953] [Train Loss 0.697671]\n",
            "Iter 40. [Val Acc 70%, Loss 0.920933] [Train Loss 0.709137]\n",
            "Iter 50. [Val Acc 69%, Loss 0.971281] [Train Loss 0.600310]\n",
            "Iter 60. [Val Acc 72%, Loss 0.969603] [Train Loss 0.543990]\n",
            "Iter 70. [Val Acc 71%, Loss 0.980826] [Train Loss 0.582862]\n",
            "Iter 80. [Val Acc 71%, Loss 0.945998] [Train Loss 0.600311]\n",
            "Iter 90. [Val Acc 70%, Loss 1.018703] [Train Loss 0.673429]\n",
            "Iter 100. [Val Acc 65%, Loss 1.020365] [Train Loss 0.757225]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([ 1.42675073e+00, -1.05957610e+00, -1.02575643e-01, -1.89365106e-01,\n",
              "        -1.23124842e-01, -7.15889675e-01,  4.34336495e-03, -4.73991609e-01,\n",
              "        -2.32087943e-01,  4.10724557e-02, -1.41876940e-01,  2.87577806e-02,\n",
              "         1.21746876e-01,  2.07813706e-01, -2.36919473e-02,  2.14427995e-01,\n",
              "        -2.52271720e-02,  2.08583600e-01, -1.03718429e-02,  3.66292682e-02,\n",
              "        -5.49574183e-02,  1.04087626e-01,  5.54480973e-02, -9.11016985e-02,\n",
              "        -1.80311232e-01, -3.89906926e-02,  1.34717411e-01, -5.27795983e-02,\n",
              "         8.12361703e-02, -7.66348419e-02,  2.40008775e-01, -1.77276029e-01,\n",
              "        -1.56331284e-01,  2.24505265e-01, -8.84266227e-02, -1.34059832e-01,\n",
              "         5.16806716e-02, -8.20176203e-02,  1.80305571e-01,  7.14545538e-02,\n",
              "        -1.31079557e-02, -8.68501698e-02,  1.07491790e-01, -3.39126983e-01,\n",
              "         1.17538747e-01, -3.73849993e-03,  1.42907085e-01, -1.76100632e-01,\n",
              "         2.90244718e-01, -9.47681923e-02,  1.04283429e-01, -1.53912259e-01,\n",
              "         7.05780213e-04,  2.99745630e-02, -9.68835846e-02,  1.38080554e-01,\n",
              "        -3.36562615e-02,  9.71029817e-02, -2.24705549e-01, -8.48389483e-02,\n",
              "         3.30409828e-03, -6.89127262e-02,  6.95816667e-02,  3.53730570e-02,\n",
              "        -2.48318185e-01,  8.57941655e-02, -6.11433117e-03,  4.52736285e-02,\n",
              "        -1.40773871e-02, -1.09209738e-01, -1.63223781e-01, -1.43653499e-03,\n",
              "        -1.14271580e-01,  6.97612001e-02,  2.47464952e-02,  8.07648232e-02,\n",
              "         9.67257861e-03, -1.27551316e-01, -1.60401571e-01, -2.12996763e-01,\n",
              "         1.65343635e-01,  2.29985676e-01,  4.87186652e-02,  1.99670150e-01,\n",
              "         3.98364607e-01, -1.73691403e-01,  6.48931685e-02, -9.72863496e-02,\n",
              "         2.18092891e-01, -1.29591114e-01]), 0.45150394651188275)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1R8xkGYrMSa8",
        "colab_type": "text"
      },
      "source": [
        "### Part (g) -- 2 pts\n",
        "\n",
        "Find the optimial value of ${\\bf w}$ and $b$ using your code. Explain how you chose\n",
        "the learning rate $\\alpha$ and the batch size."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4fjv6bUHMSa9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a7b5eefb-3b33-4358-f119-5162beda9121"
      },
      "source": [
        "w0 = np.zeros(90)\n",
        "b0 = 0.\n",
        "# Write your code here\n",
        "a = 0.0001\n",
        "max_acc = float('inf')\n",
        "best_a = None\n",
        "\n",
        "while(a <= 1):\n",
        "  \n",
        "  cur_w, cur_b = run_gradient_descent(w0,b0,alpha=a)\n",
        "  cur_y_val = pred(cur_w, cur_b, val_norm_xs)\n",
        "\n",
        "  if(get_accuracy(cur_y_val, val_ts) < min_acc):\n",
        "    min_acc = get_accuracy(cur_y_val, val_ts)\n",
        "    best_a = a\n",
        "  a = a*10\n",
        "\n",
        "print('====================')\n",
        "b = 100\n",
        "min_acc = float('inf')\n",
        "best_b = None\n",
        "\n",
        "while(b <= 1000):\n",
        "  \n",
        "  cur_w, cur_b = run_gradient_descent(w0,b0,alpha=best_a, batch_size=b)\n",
        "  cur_y_val = pred(cur_w, cur_b, val_norm_xs)\n",
        "\n",
        "  if(get_accuracy(cur_y_val, val_ts) < min_acc):\n",
        "    min_acc = get_accuracy(cur_y_val, val_ts)\n",
        "    best_b = b\n",
        "  b = b+100\n",
        "\n",
        "w, b = run_gradient_descent(w0,b0,alpha=best_a, batch_size=best_b)\n",
        "#TODO\n"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iter 10. [Val Acc 64%, Loss 0.693142] [Train Loss 0.693110]\n",
            "Iter 20. [Val Acc 65%, Loss 0.693137] [Train Loss 0.693034]\n",
            "Iter 30. [Val Acc 64%, Loss 0.693132] [Train Loss 0.692796]\n",
            "Iter 40. [Val Acc 65%, Loss 0.693128] [Train Loss 0.692701]\n",
            "Iter 50. [Val Acc 66%, Loss 0.693126] [Train Loss 0.692834]\n",
            "Iter 60. [Val Acc 66%, Loss 0.693120] [Train Loss 0.692717]\n",
            "Iter 70. [Val Acc 65%, Loss 0.693117] [Train Loss 0.692647]\n",
            "Iter 80. [Val Acc 66%, Loss 0.693113] [Train Loss 0.692657]\n",
            "Iter 90. [Val Acc 66%, Loss 0.693109] [Train Loss 0.692341]\n",
            "Iter 100. [Val Acc 66%, Loss 0.693105] [Train Loss 0.692179]\n",
            "Iter 10. [Val Acc 64%, Loss 0.693120] [Train Loss 0.692599]\n",
            "Iter 20. [Val Acc 66%, Loss 0.693083] [Train Loss 0.691814]\n",
            "Iter 30. [Val Acc 66%, Loss 0.693059] [Train Loss 0.690829]\n",
            "Iter 40. [Val Acc 66%, Loss 0.693040] [Train Loss 0.690337]\n",
            "Iter 50. [Val Acc 66%, Loss 0.693025] [Train Loss 0.688961]\n",
            "Iter 60. [Val Acc 66%, Loss 0.692984] [Train Loss 0.687179]\n",
            "Iter 70. [Val Acc 67%, Loss 0.692959] [Train Loss 0.688000]\n",
            "Iter 80. [Val Acc 66%, Loss 0.692961] [Train Loss 0.687831]\n",
            "Iter 90. [Val Acc 66%, Loss 0.692970] [Train Loss 0.685793]\n",
            "Iter 100. [Val Acc 67%, Loss 0.692963] [Train Loss 0.683913]\n",
            "Iter 10. [Val Acc 66%, Loss 0.692967] [Train Loss 0.679847]\n",
            "Iter 20. [Val Acc 66%, Loss 0.693489] [Train Loss 0.679535]\n",
            "Iter 30. [Val Acc 67%, Loss 0.693852] [Train Loss 0.688109]\n",
            "Iter 40. [Val Acc 67%, Loss 0.694655] [Train Loss 0.674834]\n",
            "Iter 50. [Val Acc 68%, Loss 0.695784] [Train Loss 0.662802]\n",
            "Iter 60. [Val Acc 68%, Loss 0.696641] [Train Loss 0.664099]\n",
            "Iter 70. [Val Acc 67%, Loss 0.697590] [Train Loss 0.649501]\n",
            "Iter 80. [Val Acc 66%, Loss 0.698652] [Train Loss 0.641814]\n",
            "Iter 90. [Val Acc 67%, Loss 0.699779] [Train Loss 0.653402]\n",
            "Iter 100. [Val Acc 68%, Loss 0.700451] [Train Loss 0.667561]\n",
            "Iter 10. [Val Acc 67%, Loss 0.704183] [Train Loss 0.657380]\n",
            "Iter 20. [Val Acc 69%, Loss 0.716920] [Train Loss 0.604662]\n",
            "Iter 30. [Val Acc 67%, Loss 0.727269] [Train Loss 0.633635]\n",
            "Iter 40. [Val Acc 70%, Loss 0.733288] [Train Loss 0.643836]\n",
            "Iter 50. [Val Acc 69%, Loss 0.741399] [Train Loss 0.614417]\n",
            "Iter 60. [Val Acc 69%, Loss 0.745995] [Train Loss 0.623470]\n",
            "Iter 70. [Val Acc 71%, Loss 0.757207] [Train Loss 0.601599]\n",
            "Iter 80. [Val Acc 71%, Loss 0.760913] [Train Loss 0.568290]\n",
            "Iter 90. [Val Acc 71%, Loss 0.765407] [Train Loss 0.602518]\n",
            "Iter 100. [Val Acc 71%, Loss 0.772371] [Train Loss 0.593763]\n",
            "Iter 10. [Val Acc 68%, Loss 0.857519] [Train Loss 0.675342]\n",
            "Iter 20. [Val Acc 64%, Loss 0.924271] [Train Loss 0.675456]\n",
            "Iter 30. [Val Acc 64%, Loss 0.948065] [Train Loss 0.570498]\n",
            "Iter 40. [Val Acc 69%, Loss 0.930875] [Train Loss 0.670519]\n",
            "Iter 50. [Val Acc 71%, Loss 0.893801] [Train Loss 0.572319]\n",
            "Iter 60. [Val Acc 70%, Loss 0.959638] [Train Loss 0.703386]\n",
            "Iter 70. [Val Acc 68%, Loss 0.984674] [Train Loss 0.656523]\n",
            "Iter 80. [Val Acc 69%, Loss 0.971344] [Train Loss 0.595403]\n",
            "Iter 90. [Val Acc 70%, Loss 0.940043] [Train Loss 0.495983]\n",
            "Iter 100. [Val Acc 70%, Loss 0.929413] [Train Loss 0.550651]\n",
            "====================\n",
            "Iter 10. [Val Acc 62%, Loss 0.693143] [Train Loss 0.693097]\n",
            "Iter 20. [Val Acc 65%, Loss 0.693139] [Train Loss 0.693045]\n",
            "Iter 30. [Val Acc 65%, Loss 0.693137] [Train Loss 0.692885]\n",
            "Iter 40. [Val Acc 65%, Loss 0.693133] [Train Loss 0.692720]\n",
            "Iter 50. [Val Acc 64%, Loss 0.693128] [Train Loss 0.692928]\n",
            "Iter 60. [Val Acc 66%, Loss 0.693124] [Train Loss 0.692750]\n",
            "Iter 70. [Val Acc 67%, Loss 0.693119] [Train Loss 0.692385]\n",
            "Iter 80. [Val Acc 66%, Loss 0.693114] [Train Loss 0.692582]\n",
            "Iter 90. [Val Acc 66%, Loss 0.693111] [Train Loss 0.692520]\n",
            "Iter 100. [Val Acc 66%, Loss 0.693108] [Train Loss 0.692754]\n",
            "Iter 10. [Val Acc 61%, Loss 0.693144] [Train Loss 0.693080]\n",
            "Iter 20. [Val Acc 64%, Loss 0.693140] [Train Loss 0.693053]\n",
            "Iter 30. [Val Acc 66%, Loss 0.693135] [Train Loss 0.692962]\n",
            "Iter 40. [Val Acc 66%, Loss 0.693131] [Train Loss 0.692702]\n",
            "Iter 50. [Val Acc 67%, Loss 0.693126] [Train Loss 0.692897]\n",
            "Iter 60. [Val Acc 67%, Loss 0.693123] [Train Loss 0.692588]\n",
            "Iter 70. [Val Acc 67%, Loss 0.693120] [Train Loss 0.692633]\n",
            "Iter 80. [Val Acc 66%, Loss 0.693117] [Train Loss 0.692682]\n",
            "Iter 90. [Val Acc 66%, Loss 0.693114] [Train Loss 0.692429]\n",
            "Iter 100. [Val Acc 67%, Loss 0.693111] [Train Loss 0.692301]\n",
            "Iter 10. [Val Acc 65%, Loss 0.693143] [Train Loss 0.693079]\n",
            "Iter 20. [Val Acc 65%, Loss 0.693139] [Train Loss 0.693028]\n",
            "Iter 30. [Val Acc 65%, Loss 0.693135] [Train Loss 0.692972]\n",
            "Iter 40. [Val Acc 66%, Loss 0.693132] [Train Loss 0.692827]\n",
            "Iter 50. [Val Acc 66%, Loss 0.693127] [Train Loss 0.692716]\n",
            "Iter 60. [Val Acc 67%, Loss 0.693122] [Train Loss 0.692750]\n",
            "Iter 70. [Val Acc 67%, Loss 0.693118] [Train Loss 0.692617]\n",
            "Iter 80. [Val Acc 67%, Loss 0.693114] [Train Loss 0.692549]\n",
            "Iter 90. [Val Acc 67%, Loss 0.693111] [Train Loss 0.692468]\n",
            "Iter 100. [Val Acc 67%, Loss 0.693108] [Train Loss 0.692537]\n",
            "Iter 10. [Val Acc 66%, Loss 0.693143] [Train Loss 0.693088]\n",
            "Iter 20. [Val Acc 67%, Loss 0.693139] [Train Loss 0.693030]\n",
            "Iter 30. [Val Acc 66%, Loss 0.693134] [Train Loss 0.692924]\n",
            "Iter 40. [Val Acc 66%, Loss 0.693129] [Train Loss 0.692921]\n",
            "Iter 50. [Val Acc 66%, Loss 0.693126] [Train Loss 0.692887]\n",
            "Iter 60. [Val Acc 66%, Loss 0.693123] [Train Loss 0.692734]\n",
            "Iter 70. [Val Acc 66%, Loss 0.693119] [Train Loss 0.692834]\n",
            "Iter 80. [Val Acc 66%, Loss 0.693115] [Train Loss 0.692507]\n",
            "Iter 90. [Val Acc 66%, Loss 0.693111] [Train Loss 0.692557]\n",
            "Iter 100. [Val Acc 66%, Loss 0.693106] [Train Loss 0.692322]\n",
            "Iter 10. [Val Acc 65%, Loss 0.693143] [Train Loss 0.693073]\n",
            "Iter 20. [Val Acc 66%, Loss 0.693139] [Train Loss 0.692974]\n",
            "Iter 30. [Val Acc 66%, Loss 0.693135] [Train Loss 0.692924]\n",
            "Iter 40. [Val Acc 67%, Loss 0.693132] [Train Loss 0.692809]\n",
            "Iter 50. [Val Acc 67%, Loss 0.693128] [Train Loss 0.692737]\n",
            "Iter 60. [Val Acc 66%, Loss 0.693124] [Train Loss 0.692751]\n",
            "Iter 70. [Val Acc 66%, Loss 0.693121] [Train Loss 0.692497]\n",
            "Iter 80. [Val Acc 66%, Loss 0.693117] [Train Loss 0.692527]\n",
            "Iter 90. [Val Acc 66%, Loss 0.693114] [Train Loss 0.692289]\n",
            "Iter 100. [Val Acc 66%, Loss 0.693110] [Train Loss 0.692367]\n",
            "Iter 10. [Val Acc 66%, Loss 0.693143] [Train Loss 0.693079]\n",
            "Iter 20. [Val Acc 67%, Loss 0.693138] [Train Loss 0.693007]\n",
            "Iter 30. [Val Acc 66%, Loss 0.693135] [Train Loss 0.692896]\n",
            "Iter 40. [Val Acc 66%, Loss 0.693130] [Train Loss 0.692822]\n",
            "Iter 50. [Val Acc 67%, Loss 0.693126] [Train Loss 0.692734]\n",
            "Iter 60. [Val Acc 66%, Loss 0.693122] [Train Loss 0.692567]\n",
            "Iter 70. [Val Acc 66%, Loss 0.693118] [Train Loss 0.692603]\n",
            "Iter 80. [Val Acc 66%, Loss 0.693115] [Train Loss 0.692523]\n",
            "Iter 90. [Val Acc 66%, Loss 0.693111] [Train Loss 0.692598]\n",
            "Iter 100. [Val Acc 66%, Loss 0.693108] [Train Loss 0.692522]\n",
            "Iter 10. [Val Acc 66%, Loss 0.693143] [Train Loss 0.693081]\n",
            "Iter 20. [Val Acc 67%, Loss 0.693139] [Train Loss 0.692992]\n",
            "Iter 30. [Val Acc 66%, Loss 0.693134] [Train Loss 0.692900]\n",
            "Iter 40. [Val Acc 66%, Loss 0.693130] [Train Loss 0.692887]\n",
            "Iter 50. [Val Acc 66%, Loss 0.693125] [Train Loss 0.692802]\n",
            "Iter 60. [Val Acc 66%, Loss 0.693121] [Train Loss 0.692674]\n",
            "Iter 70. [Val Acc 66%, Loss 0.693117] [Train Loss 0.692661]\n",
            "Iter 80. [Val Acc 66%, Loss 0.693113] [Train Loss 0.692668]\n",
            "Iter 90. [Val Acc 66%, Loss 0.693109] [Train Loss 0.692508]\n",
            "Iter 100. [Val Acc 66%, Loss 0.693105] [Train Loss 0.692387]\n",
            "Iter 10. [Val Acc 67%, Loss 0.693143] [Train Loss 0.693084]\n",
            "Iter 20. [Val Acc 67%, Loss 0.693139] [Train Loss 0.693010]\n",
            "Iter 30. [Val Acc 66%, Loss 0.693134] [Train Loss 0.692935]\n",
            "Iter 40. [Val Acc 66%, Loss 0.693131] [Train Loss 0.692827]\n",
            "Iter 50. [Val Acc 66%, Loss 0.693128] [Train Loss 0.692710]\n",
            "Iter 60. [Val Acc 66%, Loss 0.693124] [Train Loss 0.692658]\n",
            "Iter 70. [Val Acc 66%, Loss 0.693120] [Train Loss 0.692573]\n",
            "Iter 80. [Val Acc 66%, Loss 0.693115] [Train Loss 0.692640]\n",
            "Iter 90. [Val Acc 66%, Loss 0.693111] [Train Loss 0.692512]\n",
            "Iter 100. [Val Acc 66%, Loss 0.693107] [Train Loss 0.692369]\n",
            "Iter 10. [Val Acc 64%, Loss 0.693143] [Train Loss 0.693084]\n",
            "Iter 20. [Val Acc 64%, Loss 0.693140] [Train Loss 0.693001]\n",
            "Iter 30. [Val Acc 66%, Loss 0.693135] [Train Loss 0.692926]\n",
            "Iter 40. [Val Acc 66%, Loss 0.693131] [Train Loss 0.692854]\n",
            "Iter 50. [Val Acc 65%, Loss 0.693127] [Train Loss 0.692771]\n",
            "Iter 60. [Val Acc 66%, Loss 0.693124] [Train Loss 0.692811]\n",
            "Iter 70. [Val Acc 66%, Loss 0.693120] [Train Loss 0.692686]\n",
            "Iter 80. [Val Acc 66%, Loss 0.693116] [Train Loss 0.692540]\n",
            "Iter 90. [Val Acc 66%, Loss 0.693113] [Train Loss 0.692396]\n",
            "Iter 100. [Val Acc 66%, Loss 0.693108] [Train Loss 0.692367]\n",
            "Iter 10. [Val Acc 66%, Loss 0.693143] [Train Loss 0.693083]\n",
            "Iter 20. [Val Acc 67%, Loss 0.693138] [Train Loss 0.693005]\n",
            "Iter 30. [Val Acc 66%, Loss 0.693135] [Train Loss 0.692955]\n",
            "Iter 40. [Val Acc 66%, Loss 0.693131] [Train Loss 0.692851]\n",
            "Iter 50. [Val Acc 65%, Loss 0.693127] [Train Loss 0.692770]\n",
            "Iter 60. [Val Acc 66%, Loss 0.693123] [Train Loss 0.692696]\n",
            "Iter 70. [Val Acc 66%, Loss 0.693120] [Train Loss 0.692614]\n",
            "Iter 80. [Val Acc 66%, Loss 0.693116] [Train Loss 0.692596]\n",
            "Iter 90. [Val Acc 66%, Loss 0.693112] [Train Loss 0.692386]\n",
            "Iter 100. [Val Acc 66%, Loss 0.693108] [Train Loss 0.692477]\n",
            "Iter 10. [Val Acc 65%, Loss 0.693143] [Train Loss 0.693099]\n",
            "Iter 20. [Val Acc 66%, Loss 0.693138] [Train Loss 0.693046]\n",
            "Iter 30. [Val Acc 65%, Loss 0.693134] [Train Loss 0.692906]\n",
            "Iter 40. [Val Acc 66%, Loss 0.693129] [Train Loss 0.692821]\n",
            "Iter 50. [Val Acc 66%, Loss 0.693124] [Train Loss 0.692863]\n",
            "Iter 60. [Val Acc 66%, Loss 0.693121] [Train Loss 0.692845]\n",
            "Iter 70. [Val Acc 66%, Loss 0.693117] [Train Loss 0.692482]\n",
            "Iter 80. [Val Acc 66%, Loss 0.693113] [Train Loss 0.692597]\n",
            "Iter 90. [Val Acc 66%, Loss 0.693109] [Train Loss 0.692376]\n",
            "Iter 100. [Val Acc 66%, Loss 0.693105] [Train Loss 0.692172]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HjHUpMOzMSbE",
        "colab_type": "text"
      },
      "source": [
        "### Part (h) -- 4 pts\n",
        "\n",
        "Using the values of `w` and `b` from part (g), compute your training accuracy, validation accuracy,\n",
        "and test accuracy. Are there any differences between those three values? If so, why?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dzuX4T2lMSbG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "fcb3eddf-368b-4ab8-c2e4-bfc2b465880c"
      },
      "source": [
        "# Write your code here\n",
        "\n",
        "#get predictions\n",
        "y_train = pred(w,b,train_norm_xs)\n",
        "y_val = pred(w,b,val_norm_xs)\n",
        "y_test = pred(w,b,test_norm_xs)\n",
        "\n",
        "#get acc\n",
        "print('train accuracy = '+str(get_accuracy(y_train, train_ts)))\n",
        "print('validation accuracy = '+str(get_accuracy(y_val, val_ts)))\n",
        "print('train accuracy = '+str(get_accuracy(y_test, test_ts)))"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train accuracy = 0.6433100080973617\n",
            "validation accuracy = 0.64532\n",
            "train accuracy = 0.6426302537284525\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9kzd7q9MSbK",
        "colab_type": "text"
      },
      "source": [
        "### Part (i) -- 4 pts\n",
        "\n",
        "Writing a classifier like this is instructive, and helps you understand what happens when\n",
        "we train a model. However, in practice, we rarely write model building and training code\n",
        "from scratch. Instead, we typically use one of the well-tested libraries available in a package.\n",
        "\n",
        "Use `sklearn.linear_model.LogisticRegression` to build a linear classifier, and make predictions about the test set. Start by reading the\n",
        "[API documentation here](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html).\n",
        "\n",
        "Compute the training, validation and test accuracy of this model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lFAYyZdRMSbM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "8748a926-fbc4-4b87-971b-b36d92faf89f"
      },
      "source": [
        "import sklearn.linear_model as lin\n",
        "\n",
        "#init\n",
        "model = lin.LogisticRegression()\n",
        "model.fit(train_norm_xs, train_ts.flatten())\n",
        "w = model.coef_.T\n",
        "b = model.intercept_\n",
        "\n",
        "#get predictions\n",
        "y_train = pred(w,b,train_norm_xs)\n",
        "y_val = pred(w,b,val_norm_xs)\n",
        "y_test = pred(w,b,test_norm_xs)\n",
        "\n",
        "#get acc\n",
        "print('train accuracy = '+str(get_accuracy(y_train, train_ts)))\n",
        "print('validation accuracy = '+str(get_accuracy(y_val, val_ts)))\n",
        "print('train accuracy = '+str(get_accuracy(y_test, test_ts)))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train accuracy = 0.7330191073565135\n",
            "validation accuracy = 0.732\n",
            "train accuracy = 0.726864226225063\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RbXXzVCUMSbS",
        "colab_type": "text"
      },
      "source": [
        "## Part 3. Nearest Neighbour\n",
        "\n",
        "We will compare the nearest neighbour model with the model we built in the earlier parts.\n",
        "\n",
        "To make predictions for a new data point using k-nearest neighbour, we will need to:\n",
        "\n",
        "1. Compute the distance from this new data point to every element in the training set\n",
        "2. Select the top $k$ closest neighbour in the training set\n",
        "3. Find the most common label among those neighbours\n",
        "\n",
        "We'll use the validation test to select $k$. That is, we'll select the $k$ that gives the highest\n",
        "validation accuracy.\n",
        "\n",
        "Since we have a fairly large data set, computing the distance between a point in the validation\n",
        "set and all points in the training set will require more RAM than Google Colab provides.\n",
        "To make the comptuations tractable, we will:\n",
        "\n",
        "1. Use only a subset of the training set (only the first 100,000 elements)\n",
        "2. Use only a subset of the validation set (only the first 1000 elements)\n",
        "3. We will use the **cosine similarity** rather than Euclidean distance. We will also pre-scale\n",
        "   each element in training set and the validation set to be a unit vector, so that computing\n",
        "   the cosine similarity is equivalent to computing the dot product. To see this, recall that \n",
        "   $$cos(\\theta) = \\frac{v \\cdot w}{||v|| ||w||}$$. But if both ||v|| and ||w|| are zero, then\n",
        "   only the dot product remains."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YLNRBAijMSbT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# we'll need to take the first 100000 element of `train_norm_xs`\n",
        "# and scale each of its rows to be unit length\n",
        "xs = train_norm_xs[:100000]\n",
        "# compute the norms:\n",
        "norms = np.linalg.norm(xs, axis=1)\n",
        "# divide the xs by the norms. Because of numpy's broadcasting rules, we need to\n",
        "# transpose the matrices a couple of times:\n",
        "#   https://docs.scipy.org/doc/numpy/user/basics.broadcasting.html\n",
        "xs = (xs.T / norms).T"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDRsOHQPMSbX",
        "colab_type": "text"
      },
      "source": [
        "### Part (a) -- 1 pt\n",
        "\n",
        "Create a numpy matrix `val_xs` that contains the first 1000 elements of `val_norm_xs`, scaled\n",
        "so that each of its rows is unit length. Follow the code above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_2f78lIVMSbY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# val_xs = ..."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3aUtFdDMSbe",
        "colab_type": "text"
      },
      "source": [
        "### Part (b) -- 1 pt\n",
        "\n",
        "Our goal now is to compute the validation accuracy for a choice of $k$. This will\n",
        "require computing the distance between each song in the training set and each\n",
        "song in the validation set.\n",
        "\n",
        "This is actually quite straightforward, and can be done using one matrix\n",
        "computation operation!\n",
        "\n",
        "Compute all the distances between elements of `xs` and those of `val_xs`\n",
        "using a single call to `np.dot`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TSjqw2BRMSbg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#val_distances = np.dot ..."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMYC__8pMSbl",
        "colab_type": "text"
      },
      "source": [
        "### Part (c) -- 3 pt\n",
        "\n",
        "Now that we have the distance pairs, we can use the matrix `val_distances`\n",
        "to find the set of neighbours for each point in our validation set and \n",
        "\n",
        "Find the validation accuracy assuming that we use $k = 10$. You may\n",
        "use the below helper function if you want, and the `get_accuracy` helper\n",
        "from the last section.\n",
        "\n",
        "You might also find it helpful to do parts (c) and (d) together."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44FnZ52-MSbo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_nearest_neighbours(i, k=10):\n",
        "  \"\"Return the indices of the top k-element of `xs` that are closests to\n",
        "  element `i` of the validation set `val_xs`.\n",
        "  \"\"\n",
        "  # sort the element of the training set by distance to the i-th\n",
        "  # element of val_xs\n",
        "  neighbours = sorted(enumerate(val_distances[:, i]),\n",
        "                      key=lambda r: r[1],\n",
        "                      reverse=True)\n",
        "  # obtain the top k closest index and return it\n",
        "  neighbour_indices = [index for (index, dist) in neighbours[:k]]\n",
        "  return neighbour_indices \n",
        "\n",
        "def get_train_ts(indices):\n",
        "  \"\"\"Return the labels of the corresponding elements in the training set `xs`.\n",
        "  Note that `xs` is the first 100,000 elements of `train_xs`, so we can\n",
        "  simply index `train_ts`.\n",
        "  \"\"\"\n",
        "  return train_ts[indices]\n",
        "\n",
        "# Write your code here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTljGjJYMSbt",
        "colab_type": "text"
      },
      "source": [
        "### Part (d) -- 2 pts\n",
        "\n",
        "Compute the validation accuracy for $k = 50, 100, and 1000$.\n",
        "Which $k$ provides the best results? In other words, which kNN model would you deploy?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zIp6P5RfMSbu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Write your code and solution here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSpEcZ_lMSby",
        "colab_type": "text"
      },
      "source": [
        "### Part (e) -- 4 pt\n",
        "\n",
        "Compute the test accuracy for the $k$ that you chose in the previous part.\n",
        "Use only a sample of 1000 elements from the test set to keep the problem tractable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x6eZg-eWMSbz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Write your code and solution here"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}