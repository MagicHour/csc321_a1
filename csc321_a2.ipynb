{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "csc321_a2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MagicHour/csc321_a1/blob/master/csc321_a2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VgimEp7IsDA2",
        "colab_type": "text"
      },
      "source": [
        "# CSC321H5 Project 2.\n",
        "\n",
        "**Deadline**: Thursday, Feb. 13, by 9pm\n",
        "\n",
        "**Submission**: Submit a PDF export of the completed notebook. \n",
        "\n",
        "**Late Submission**: Please see the syllabus for the late submission criteria.\n",
        "\n",
        "Based on an assignment by George Dahl, Jing Yao Li, and Roger Grosse\n",
        "\n",
        "In this assignment, we will make a neural network that can predict the next word\n",
        "in a sentence given the previous three. \n",
        "We'll explore a couple of different models to perform this prediction task. We will also do this\n",
        "problem twice: once in PyTorch, and once using numpy. When using numpy, you'll implement\n",
        "the backpropagation computation.\n",
        "\n",
        "In doing this prediction task, our neural networks will learn about *words* and about\n",
        "how to represent words. We'll explore the *vector representations* of words that our\n",
        "model produces, and analyze these representations.\n",
        "\n",
        "You may modify the starter code as you see fit, including changing the signatures of\n",
        "functions and adding/removing helper functions. However, please make sure that your\n",
        "TA can understand what you are doing and why."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0O6d0qEsDA-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F9g1eX3QsDBR",
        "colab_type": "text"
      },
      "source": [
        "## Question 1. Data\n",
        "\n",
        "With any machine learning problem, the first thing that we would want to do\n",
        "is to get an intuitive understanding of what our data looks like. Download the file\n",
        "`raw_sentences.txt` from `https://www.cs.toronto.edu/~lczhang/321/hw/raw_sentences.txt`\n",
        "and upload it to Google Drive.\n",
        "Then, mount Google Drive from your Google Colab notebook:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRN6paNBsDBU",
        "colab_type": "code",
        "outputId": "9e3603d0-bf52-459a-aafb-256b8a733d6b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_7cApKvsDBh",
        "colab_type": "text"
      },
      "source": [
        "Find the path to `raw_sentences.txt`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FA-6IOmcsDBk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file_path = '/content/gdrive/My Drive/CSC321/raw_sentences.txt' # TODO - UPDATE ME!"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6MudcKisDBv",
        "colab_type": "text"
      },
      "source": [
        "You might find it helpful to know that you can run shell commands (like `ls`) by\n",
        "using `!` in Google Colab, like this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uau8M38gsDBy",
        "colab_type": "code",
        "outputId": "b2f1f233-aa36-43ae-cc86-c42c7a721adf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!ls /content/gdrive/My\\ Drive/\n",
        "#!mkdir /content/gdrive/My\\ Drive/CSC321"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " 20151221_115249.jpg\t        20160323_161616.jpg\n",
            " 20151221_115255.jpg\t        20160323_161628.jpg\n",
            " 20151221_115258.jpg\t        20160323_161636.jpg\n",
            " 20151221_115434.jpg\t        20160323_163041.jpg\n",
            " 20151221_115503.jpg\t        20160323_163049.jpg\n",
            " 20151221_115706.jpg\t        20160323_163953.jpg\n",
            " 20151221_122424.jpg\t        20160323_164002.jpg\n",
            " 20151221_122428.jpg\t        20160412_203442.jpg\n",
            " 20151221_122440.jpg\t        20160412_203505.jpg\n",
            " 20151221_122501.jpg\t        20160412_203519.jpg\n",
            " 20151221_122511.jpg\t        20160414_093456.jpg\n",
            " 20151221_122722.jpg\t        20160414_093526.jpg\n",
            " 20151221_122732.jpg\t        20160522_140741.jpg\n",
            " 20151221_122752.jpg\t        20160522_140808.jpg\n",
            " 20151221_150538.jpg\t        20160522_140817.jpg\n",
            " 20151221_150559.jpg\t        20160527_152137.jpg\n",
            " 20151226_192200.jpg\t        20160527_152555.jpg\n",
            " 20151226_192205.jpg\t        20160529_194828.jpg\n",
            " 20151226_192210.jpg\t        20160529_194846.jpg\n",
            " 20151226_192233.jpg\t        20160625_122446.jpg\n",
            " 20151228_094240.jpg\t        20160625_122505.jpg\n",
            " 20151230_201907.jpg\t        20160625_123823.jpg\n",
            " 20151230_201927.jpg\t        20160627_103918.jpg\n",
            " 20151230_201950.jpg\t        20160627_103927.jpg\n",
            " 20151230_202041.jpg\t        20160627_103939.jpg\n",
            " 20151230_202051.jpg\t        20160627_104000.jpg\n",
            " 20151230_202052.jpg\t        20160627_105307.jpg\n",
            " 20151230_202216.jpg\t        20160627_105316.jpg\n",
            " 20151230_202326.jpg\t        20160701_105645.jpg\n",
            " 20151230_203208.jpg\t        20160701_105648_001.jpg\n",
            " 20151230_203224.jpg\t        20160701_105648_002.jpg\n",
            " 20151230_203248.jpg\t        20160701_105648_003.jpg\n",
            " 20151230_203315.jpg\t        20160701_105648_004.jpg\n",
            " 20151230_203355.jpg\t        20160701_105648_005.jpg\n",
            " 20151230_203407.jpg\t        20160701_105648_006.jpg\n",
            " 20151230_203425.jpg\t        20160701_105703.jpg\n",
            " 20151230_203430.jpg\t        20160701_105756.jpg\n",
            " 20151230_203449.jpg\t        20160701_105758.jpg\n",
            " 20151230_203523.jpg\t        20160701_105818.jpg\n",
            " 20151230_203538.jpg\t        20160701_105825.jpg\n",
            " 20151230_203604.jpg\t        20160701_105851.jpg\n",
            " 20151230_203637.jpg\t        20160701_105853.jpg\n",
            " 20151230_204906.jpg\t        20160912_174729.jpg\n",
            " 20151230_205016.jpg\t        20160929_141248.jpg\n",
            " 20151231_175601.jpg\t        20160929_141250.jpg\n",
            " 20151231_175605.jpg\t        20160929_141319.jpg\n",
            " 20151231_175611.jpg\t        20161129_212538.jpg\n",
            " 20151231_181113.jpg\t        20161129_212546.jpg\n",
            " 20151231_181143.jpg\t        20161129_212551.jpg\n",
            " 20151231_181729.jpg\t        20161205_202229.jpg\n",
            " 20151231_182357.jpg\t        20161205_202234.jpg\n",
            " 20151231_182507.jpg\t        20161213_224013.jpg\n",
            " 20151231_183602.jpg\t        20161226_221856.jpg\n",
            " 20151231_183718.jpg\t        20161226_223235.jpg\n",
            " 20151231_183739.jpg\t        20170408_211946.jpg\n",
            " 20160101_111445.jpg\t        20170408_211954.jpg\n",
            "'2016-01-01 13.09.00 (1).jpg'   20170506_095330.jpg\n",
            "'2016-01-01 13.09.00.jpg'       20170617_144700.jpg\n",
            "'2016-01-01 13.09.50 (1).jpg'   20170627_095236.jpg\n",
            "'2016-01-01 13.09.50.jpg'       20170731_204708.jpg\n",
            " 20160102_175852.jpg\t        20170801_183823.jpg\n",
            " 20160102_180944.jpg\t        20170802_182730.jpg\n",
            " 20160102_180946.jpg\t        20170810_084718.jpg\n",
            " 20160102_181109.jpg\t       'Colab Notebooks'\n",
            " 20160102_184136.jpg\t       'Consulting proposal.gslides'\n",
            " 20160102_184255.jpg\t        CSC321\n",
            " 20160102_184925.jpg\t       'csc343 a3.gdoc'\n",
            " 20160102_184934.jpg\t       'HRE3M1courseoutline (1).pdf'\n",
            " 20160209_151635.jpg\t        HRE3M1courseoutline-1.pdf\n",
            " 20160226_123904.jpg\t       'HRE3M1courseoutline (2).pdf'\n",
            " 20160226_123917.jpg\t       'HRE3M1courseoutline (3).pdf'\n",
            " 20160323_152201.jpg\t        HRE3M1courseoutline.pdf\n",
            " 20160323_152406.jpg\t       'Louis Armstrong - La Vie En Rose - SoundCloud'\n",
            " 20160323_152522.jpg\t       'Randy MRI'\n",
            " 20160323_153148.jpg\t       'Sea_of_Stories_Allegory (1).doc.pdf'\n",
            " 20160323_153800.jpg\t       'Sea_of_Stories_Allegory (2).doc.pdf'\n",
            " 20160323_154726.jpg\t       'Sea_of_Stories_Allegory (3).doc.pdf'\n",
            " 20160323_154855.jpg\t        Sea_of_Stories_Allegory.doc.pdf\n",
            " 20160323_155535.jpg\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMKBG6yhsDB9",
        "colab_type": "text"
      },
      "source": [
        "The following code reads the sentences in our file, split each sentence into\n",
        "its individual words, and stores the sentences (list of words) in the\n",
        "variable `sentences`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "czUHB83QsDCA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentences = []\n",
        "for line in open(file_path):\n",
        "    words = line.split()\n",
        "    sentence = [word.lower() for word in words]\n",
        "    sentences.append(sentence)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0YaHUlGosDCK",
        "colab_type": "text"
      },
      "source": [
        "There are 97,162 sentences in total, and \n",
        "these sentences are composed of 250 distinct words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQhWKI_tsDCQ",
        "colab_type": "code",
        "outputId": "9c4c5d31-174e-48ce-8dd3-ebe7f66aa027",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "vocab = set([w for s in sentences for w in s])\n",
        "print(len(sentences)) # 97162\n",
        "print(len(vocab)) # 250"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "97162\n",
            "250\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-3G6AeosDCb",
        "colab_type": "text"
      },
      "source": [
        "We'll separate our data into training, validation, and test.\n",
        "We'll use `10,000 sentences for test, 10,000 for validation, and\n",
        "the rest for training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHmgsN_usDCe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test, valid, train = sentences[:10000], sentences[10000:20000], sentences[20000:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6dN4GBCsDCn",
        "colab_type": "text"
      },
      "source": [
        "### Part (a) -- 2 pts\n",
        "\n",
        "Display 10 sentences in the training set.\n",
        "Explain how punctuations are treated in our word representation, and how words\n",
        "with apostrophes are represented."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i1nUsdBmsDCr",
        "colab_type": "code",
        "outputId": "df58c56b-5414-4b63-a607-687ba25094db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "# Your code goes here\n",
        "print(sentences[:10])\n",
        "#Words with apostrophes are represented as 2 words, the first half being the\n",
        "#base word, the second half being the contraction including the apostrophe."
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['no', ',', 'he', 'says', 'now', '.'], ['and', 'what', 'did', 'he', 'do', '?'], ['the', 'money', \"'s\", 'there', '.'], ['that', 'was', 'less', 'than', 'a', 'year', 'ago', '.'], ['but', 'he', 'made', 'only', 'the', 'first', '.'], ['there', \"'s\", 'still', 'time', 'for', 'them', 'to', 'do', 'it', '.'], ['but', 'he', 'should', 'nt', 'have', '.'], ['they', 'have', 'to', 'come', 'down', 'to', 'the', 'people', '.'], ['i', 'do', 'nt', 'know', 'where', 'that', 'is', '.'], ['no', ',', 'i', 'would', 'nt', '.']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-esgFlhtsDC3",
        "colab_type": "text"
      },
      "source": [
        "### Part (b) -- 2 pts\n",
        "\n",
        "What are the 10 most common words in the vocabulary? How often does each of these\n",
        "words appear in the training sentences? Express the second quantity a percentage\n",
        "(i.e. number of occurences of the  word / total number of words in the training set).\n",
        "\n",
        "These are good quantities to compute, because one of the first things a machine learning\n",
        "model will learn is to predict the **most common** class. Getting a sense of the\n",
        "distribution of our data will help you understand our model's behaviour.\n",
        "\n",
        "You can use Python's `collections.Counter` class if you would like to."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lBOqB6klsDC7",
        "colab_type": "code",
        "outputId": "ad3d5d4d-aee5-4414-f8d9-dc59aa2f32c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        }
      },
      "source": [
        "# Your code goes here\n",
        "from collections import Counter\n",
        "\n",
        "count = Counter()\n",
        "total_words = 0\n",
        "for sentence in sentences:\n",
        "  for word in sentence:\n",
        "    count[word] += 1\n",
        "    total_words += 1\n",
        "\n",
        "for word, num in count.most_common(10):\n",
        "  print(word + \"  :\" + str((num/float(total_words))*100) + \"%\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".  :10.694267616869087%\n",
            "it  :3.856455336436107%\n",
            ",  :3.2466863539592064%\n",
            "i  :2.9408113348089997%\n",
            "do  :2.673765009799623%\n",
            "to  :2.5802591749298704%\n",
            "nt  :2.173878590333504%\n",
            "?  :2.1408609932749756%\n",
            "the  :2.1050699180635313%\n",
            "that  :2.086051782157819%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52qwmLwpsDDE",
        "colab_type": "text"
      },
      "source": [
        "### Part (c) -- 4 pts\n",
        "\n",
        "Complete the helper functions `convert_words_to_indices` and\n",
        "`generate_4grams`, so that the function `process_data` will take a \n",
        "list of sentences (i.e. list of list of words), and generate an \n",
        "$$N \\times 4$$ numpy matrix containing indices of 4 words that appear\n",
        "next to each other. You can use the constances `vocab`, `vocab_itos`,\n",
        "and `vocab_stoi` in your code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EdnhwXzvsDDG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# A list of all the words in the data set. We will assign a unique \n",
        "# identifier for each of these words.\n",
        "vocab = sorted(list(set([w for s in train for w in s])))\n",
        "# A mapping of index => word (string)\n",
        "vocab_itos = dict(enumerate(vocab))\n",
        "# A mapping of word => its index\n",
        "vocab_stoi = {word:index for index, word in vocab_itos.items()}\n",
        "\n",
        "def convert_words_to_indices(sents):\n",
        "    \"\"\"\n",
        "    This function takes a list of sentences (list of list of words)\n",
        "    and returns a new list with the same structure, but where each word\n",
        "    is replaced by its index in `vocab_stoi`.\n",
        "\n",
        "    Example:\n",
        "    >>> convert_words_to_indices([['one', 'in', 'five', 'are', 'over', 'here'], ['other', 'one', 'since', 'yesterday'], ['you']])\n",
        "    [[148, 98, 70, 23, 154, 89], [151, 148, 181, 246], [248]]\n",
        "    \"\"\"\n",
        "    \n",
        "    #convert a single sentence into a list of corresponding indices\n",
        "    def sentence_to_indices(sent):\n",
        "      a = []\n",
        "      for word in sent:\n",
        "        a.append(vocab_stoi[word])\n",
        "      return a\n",
        "\n",
        "    # Write your code here\n",
        "    return list(map(sentence_to_indices, sents))\n",
        "\n",
        "def generate_4grams(seqs):\n",
        "    \"\"\"\n",
        "    This function takes a list of sentences (list of lists) and returns\n",
        "    a new list containing the 4-grams (four consequentively occuring words)\n",
        "    that appear in the sentences. Note that a unique 4-gram can appear multiple\n",
        "    times, one per each time that the 4-gram appears in the data parameter `seqs`.\n",
        "\n",
        "    Example:\n",
        "\n",
        "    >>> generate_4grams([[148, 98, 70, 23, 154, 89], [151, 148, 181, 246], [248]])\n",
        "    [[148, 98, 70, 23], [98, 70, 23, 154], [70, 23, 154, 89], [151, 148, 181, 246]]\n",
        "    >>> generate_4grams([[1, 1, 1, 1, 1]])\n",
        "    [[1, 1, 1, 1], [1, 1, 1, 1]]\n",
        "    \"\"\"\n",
        "\n",
        "    # Write your code here\n",
        "    total = []\n",
        "    for sentence in seqs:\n",
        "      for i in range(0, len(sentence)-3):\n",
        "        total.append(sentence[i:i+4])\n",
        "    \n",
        "    return total\n",
        "def process_data(sents):\n",
        "    \"\"\"\n",
        "    This function takes a list of sentences (list of lists), and generates an\n",
        "    numpy matrix with shape [N, 4] containing indices of words in 4-grams.\n",
        "    \"\"\"\n",
        "    indices = convert_words_to_indices(sents)\n",
        "    fourgrams = generate_4grams(indices)\n",
        "    return np.array(fourgrams)\n",
        "\n",
        "train4grams = process_data(train)\n",
        "valid4grams = process_data(valid)\n",
        "test4grams = process_data(test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65f2D2hBsDDW",
        "colab_type": "text"
      },
      "source": [
        "## Question 2. A Multi-Layer Perceptron\n",
        "\n",
        "In this section, we will build a two-layer multi-layer perceptron.\n",
        "We will first do this in numpy, and then once more in PyTorch.\n",
        "Our model will look like this:\n",
        "\n",
        "<img src=\"https://www.cs.toronto.edu/~lczhang/321/hw/p2_model1.png\" />\n",
        "\n",
        "Start by reviewing these helper functions, which are given to you:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQPmrXhLsDDZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_onehot(indicies, total=250):\n",
        "    \"\"\"\n",
        "    Convert indicies into one-hot vectors by\n",
        "        1. Creating an identity matrix of shape [total, total]\n",
        "        2. Indexing the appropriate columns of that identity matrix\n",
        "    \"\"\"\n",
        "    I = np.eye(total)\n",
        "    return I[indicies]\n",
        "\n",
        "def softmax(x):\n",
        "    \"\"\"\n",
        "    Compute the softmax of vector x, or row-wise for a matrix x.\n",
        "    We subtract x.max(axis=0) from each row for numerical stability.\n",
        "    \"\"\"\n",
        "    x = x.T\n",
        "    exps = np.exp(x - x.max(axis=0))\n",
        "    probs = exps / np.sum(exps, axis=0)\n",
        "    return probs.T\n",
        "\n",
        "def get_batch(data, range_min, range_max, onehot=True):\n",
        "    \"\"\"\n",
        "    Convert one batch of data in the form of 4-grams into input and output\n",
        "    data and return the training data (xs, ts) where:\n",
        "     - `xs` is an numpy array of one-hot vectors of shape [batch_size, 3, 250]\n",
        "     - `ts` is either\n",
        "            - a numpy array of shape [batch_size, 250] if onehot is True,\n",
        "            - a numpy array of shape [batch_size] containing indicies otherwise\n",
        "\n",
        "    Preconditions:\n",
        "     - `data` is a numpy array of shape [N, 4] produced by a call\n",
        "        to `process_data`\n",
        "     - range_max > range_min\n",
        "    \"\"\"\n",
        "    xs = data[range_min:range_max, :3]\n",
        "    xs = make_onehot(xs)\n",
        "    ts = data[range_min:range_max, 3]\n",
        "    if onehot:\n",
        "        ts = make_onehot(ts).reshape(-1, 250)\n",
        "    return xs, ts\n",
        "\n",
        "def estimate_accuracy(model, data, batch_size=5000, max_N=100000):\n",
        "    \"\"\"\n",
        "    Estimate the accuracy of the model on the data. To reduce\n",
        "    computation time, use at most `max_N` elements of `data` to\n",
        "    produce the estimate.\n",
        "    \"\"\"\n",
        "    correct = 0\n",
        "    N = 0\n",
        "    for i in range(0, data.shape[0], batch_size):\n",
        "        xs, ts = get_batch(data, i, i + batch_size, onehot=False)\n",
        "        y = model(xs)\n",
        "        pred = np.argmax(y, axis=1)\n",
        "        correct += np.sum(ts == pred)\n",
        "        N += ts.shape[0]\n",
        "\n",
        "        if N > max_N:\n",
        "            break\n",
        "    return correct / N"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EquUpUkJsDDh",
        "colab_type": "text"
      },
      "source": [
        "### Part (a) -- 2 point\n",
        "\n",
        "Your first task is to implement MLP model in Numpy.\n",
        "This model is very similar to the one we built in Tutorial 5. However, we will\n",
        "write our code differently from Tutorial 5, so that the class methods and APIs\n",
        "are similar to that of PyTorch. This is to give you some intuition about what\n",
        "PyTorch is doing under the hood.\n",
        "\n",
        "We already wrote code for the backward pass for this model in Tutorial 5, so the\n",
        "code is given to you. To make sure you understand how the model works, \n",
        "**write the code to compute the forward pass**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CvRKRTx0sDDj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NumpyMLPModel(object):\n",
        "    def __init__(self, num_features=250*3, num_hidden=400, num_classes=250):\n",
        "        \"\"\"\n",
        "        Initialize the weights and biases of this two-layer MLP.\n",
        "        \"\"\"\n",
        "        self.num_features = num_features\n",
        "        self.num_hidden = num_hidden\n",
        "        self.num_classes = num_classes\n",
        "        self.weights1 = np.zeros([num_hidden, num_features])\n",
        "        self.bias1 = np.zeros([num_hidden])\n",
        "        self.weights2 = np.zeros([num_classes, num_hidden])\n",
        "        self.bias2 = np.zeros([num_classes])\n",
        "        self.cleanup()\n",
        "\n",
        "    def initializeParams(self):\n",
        "        \"\"\"\n",
        "        Initialize the weights and biases of this two-layer MLP to be random.\n",
        "        This random initialization is necessary to break the symmetry in the\n",
        "        gradient descent update for our hidden weights and biases. If all our\n",
        "        weights were initialized to the same value, then their gradients will\n",
        "        all be the same!\n",
        "        \"\"\"\n",
        "        self.weights1 = np.random.normal(0, 2/self.num_features, self.weights1.shape)\n",
        "        self.bias1 = np.random.normal(0, 2/self.num_features, self.bias1.shape)\n",
        "        self.weights2 = np.random.normal(0, 2/self.num_hidden, self.weights2.shape)\n",
        "        self.bias2 = np.random.normal(0, 2/self.num_hidden, self.bias2.shape)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        \"\"\"\n",
        "        Compute the forward pass prediction for inputs.\n",
        "        Note that `inputs` will be a rank-3 numpy array with shape [N, 3, 250],\n",
        "        so we will need to flatten the tensor to [N, 750] first.\n",
        "\n",
        "        For the ReLU activation, you may find the function `np.maximum` helpful\n",
        "        \"\"\"\n",
        "        X = inputs.reshape([-1, 750])\n",
        "\n",
        "        # TODO:\n",
        "        \"\"\"\n",
        "        Weights1: num_hidden x num_features\n",
        "        X: num_inputs x num_features\n",
        "        z1: num_inputs x num_hidden\n",
        "        \n",
        "        weights2: num_classes x num_hidden\n",
        "        z2: num_inputs x num_classes\n",
        "        \"\"\"\n",
        "\n",
        "        self.N = X.shape[0]\n",
        "        self.X = X\n",
        "        big_bias1 = np.tile(self.bias1, (self.N, 1))\n",
        "        self.z1 = np.matmul(self.X, self.weights1.T)+big_bias1\n",
        "        test = np.zeros(self.z1.shape)\n",
        "        self.h = np.maximum(test, self.z1) #Applying relu\n",
        "        big_bias2 = np.tile(self.bias2, (self.N, 1))\n",
        "        self.z2 = np.matmul(self.h, self.weights2.T)+big_bias2\n",
        "        self.y = softmax(self.z2)\n",
        "        return self.y\n",
        "\n",
        "    def __call__(self, inputs):\n",
        "        \"\"\"\n",
        "        To be compatible with PyTorch API. With this code, the following two\n",
        "        calls are identical:\n",
        "\n",
        "        >>> m = TwoLayerMLP()\n",
        "        >>> m.forward(inputs)\n",
        "\n",
        "        and \n",
        "\n",
        "        >>> m = TwoLayerMLP()\n",
        "        >>> m(inputs)\n",
        "        \"\"\"\n",
        "        return self.forward(inputs)\n",
        "\n",
        "    def backward(self, ts):\n",
        "        \"\"\"\n",
        "        Compute the backward pass, given the ground-truth, one-hot targets.\n",
        "        Note that `ts` needs to be a rank 2 numpy array with shape [N, 250].\n",
        "        \"\"\"\n",
        "        self.z2_bar = (self.y - ts) / self.N\n",
        "        self.w2_bar = np.dot(self.z2_bar.T, self.h)\n",
        "        self.b2_bar = np.dot(self.z2_bar.T, np.ones(self.N))\n",
        "        self.h_bar = np.matmul(self.z2_bar, self.weights2)\n",
        "        self.z1_bar = self.h_bar * (self.z1 > 0)\n",
        "        self.w1_bar = np.dot(self.z1_bar.T, self.X)\n",
        "        self.b1_bar = np.dot(self.z1_bar.T, np.ones(self.N))\n",
        "\n",
        "    def update(self, alpha):\n",
        "        \"\"\"\n",
        "        Compute the gradient descent update for the parameters.\n",
        "        \"\"\"\n",
        "        self.weights1 = self.weights1 - alpha * self.w1_bar\n",
        "        self.bias1    = self.bias1    - alpha * self.b1_bar\n",
        "        self.weights2 = self.weights2 - alpha * self.w2_bar\n",
        "        self.bias2    = self.bias2    - alpha * self.b2_bar\n",
        "\n",
        "    def cleanup(self):\n",
        "        \"\"\"\n",
        "        Erase the values of the variables that we use in our computation.\n",
        "        \"\"\"\n",
        "        self.N = None\n",
        "        self.X = None\n",
        "        self.z1 = None\n",
        "        self.h = None\n",
        "        self.z2 = None\n",
        "        self.y = None\n",
        "        self.z2_bar = None\n",
        "        self.w2_bar = None\n",
        "        self.b2_bar = None\n",
        "        self.h_bar = None\n",
        "        self.z1_bar = None\n",
        "        self.w1_bar = None\n",
        "        self.b1_bar = None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGkSo2o4sDDr",
        "colab_type": "text"
      },
      "source": [
        "### Part (b) -- 2 points\n",
        "\n",
        "Complete the `run_gradient_descent` function. Train your numpy MLP model \n",
        "to obtain a training accuracy of at least 25%. You do not need to train\n",
        "this model to convergence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4oqbacIFsDDt",
        "colab_type": "code",
        "outputId": "932276c7-6756-4495-a9d2-28841c8fc66b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        }
      },
      "source": [
        "def run_gradient_descent(model,\n",
        "                         train_data=train4grams,\n",
        "                         validation_data=valid4grams,\n",
        "                         batch_size=100,\n",
        "                         learning_rate=0.1,\n",
        "                         max_iters=5000):\n",
        "    \"\"\"\n",
        "    Use gradient descent to train the numpy model on the dataset train4grams.\n",
        "    \"\"\"\n",
        "    n = 0\n",
        "    while n < max_iters:\n",
        "        # shuffle the training data, and break early if we don't have\n",
        "        # enough data to remaining in the batch\n",
        "        np.random.shuffle(train_data)\n",
        "        for i in range(0, train_data.shape[0], batch_size):\n",
        "            if (i + batch_size) > train_data.shape[0]:\n",
        "                break\n",
        "\n",
        "            # get the input and targets of a minibatch\n",
        "            xs, ts = get_batch(train_data, i, i + batch_size, onehot=True)\n",
        "\n",
        "            # forward pass: compute prediction\n",
        "\n",
        "            # TODO: add your code here\n",
        "            y = model.forward(xs)\n",
        "\n",
        "            # backward pass: compute error \n",
        "            \n",
        "            # TODO: add your code here\n",
        "            model.backward(ts)\n",
        "            model.update(learning_rate)\n",
        "            model.cleanup()\n",
        "\n",
        "            # increment the iteration count\n",
        "            n += 1\n",
        "\n",
        "            # compute and plot the *validation* loss and accuracy\n",
        "            if (n % 100 == 0):\n",
        "                train_cost = -np.sum(ts * np.log(y)) / batch_size\n",
        "                #train_acc = estimate_accuracy(model, train_data)\n",
        "                #val_acc = estimate_accuracy(model, validation_data)\n",
        "                train_acc = estimate_accuracy(model, train_data, max_N=100)\n",
        "                val_acc = estimate_accuracy(model, validation_data, max_N=100)\n",
        "                model.cleanup()\n",
        "                print(\"Iter %d. [Val Acc %.0f%%] [Train Acc %.0f%%, Loss %f]\" % (\n",
        "                      n, val_acc * 100, train_acc * 100, train_cost))\n",
        "\n",
        "            if n >= max_iters:\n",
        "                return\n",
        "\n",
        "\n",
        "numpy_mlp = NumpyMLPModel()\n",
        "numpy_mlp.initializeParams()\n",
        "run_gradient_descent(numpy_mlp, batch_size=100, max_iters=2000, learning_rate=0.5)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iter 100. [Val Acc 17%] [Train Acc 16%, Loss 4.504667]\n",
            "Iter 200. [Val Acc 17%] [Train Acc 16%, Loss 4.412254]\n",
            "Iter 300. [Val Acc 18%] [Train Acc 17%, Loss 4.097243]\n",
            "Iter 400. [Val Acc 21%] [Train Acc 19%, Loss 4.234133]\n",
            "Iter 500. [Val Acc 22%] [Train Acc 21%, Loss 3.991362]\n",
            "Iter 600. [Val Acc 23%] [Train Acc 22%, Loss 4.230258]\n",
            "Iter 700. [Val Acc 23%] [Train Acc 23%, Loss 3.755947]\n",
            "Iter 800. [Val Acc 25%] [Train Acc 24%, Loss 3.836391]\n",
            "Iter 900. [Val Acc 25%] [Train Acc 24%, Loss 3.690485]\n",
            "Iter 1000. [Val Acc 26%] [Train Acc 25%, Loss 4.005043]\n",
            "Iter 1100. [Val Acc 25%] [Train Acc 25%, Loss 3.235973]\n",
            "Iter 1200. [Val Acc 26%] [Train Acc 26%, Loss 3.606198]\n",
            "Iter 1300. [Val Acc 26%] [Train Acc 26%, Loss 3.479657]\n",
            "Iter 1400. [Val Acc 27%] [Train Acc 26%, Loss 3.258295]\n",
            "Iter 1500. [Val Acc 27%] [Train Acc 27%, Loss 3.533569]\n",
            "Iter 1600. [Val Acc 28%] [Train Acc 27%, Loss 3.408738]\n",
            "Iter 1700. [Val Acc 28%] [Train Acc 27%, Loss 3.349905]\n",
            "Iter 1800. [Val Acc 28%] [Train Acc 27%, Loss 3.213809]\n",
            "Iter 1900. [Val Acc 29%] [Train Acc 27%, Loss 3.551513]\n",
            "Iter 2000. [Val Acc 29%] [Train Acc 27%, Loss 3.200248]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gr7rGeojsDDz",
        "colab_type": "text"
      },
      "source": [
        "### Part (c) -- 2 pts\n",
        "\n",
        "We will do build the same model in PyTorch. Since PyTorch uses automatic\n",
        "differentiation, we only need to write the *forward pass* of our\n",
        "model. Complete the `forward` function below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LJCT1cdhsDD1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PyTorchMLP(nn.Module):\n",
        "    def __init__(self, num_hidden=400):\n",
        "        super(PyTorchMLP, self).__init__()\n",
        "        self.layer1 = nn.Linear(750, num_hidden)\n",
        "        self.layer2 = nn.Linear(num_hidden, 250)\n",
        "        self.num_hidden = num_hidden\n",
        "    def forward(self, inp):\n",
        "        inp = inp.reshape([-1, 750])\n",
        "        # TODO: complete this function\n",
        "\n",
        "        #Forward pass into first layer\n",
        "        z1 = torch.relu(self.layer1(inp))\n",
        "        #Pass into second layer\n",
        "        return self.layer2(z1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_hnEWbfSsDD8",
        "colab_type": "text"
      },
      "source": [
        "### Part (d) -- 4 pts\n",
        "\n",
        "We'll write similar code to train the PyTorch model. With a few differences:\n",
        "\n",
        "1. We will use a slightly fancier optimizer called **Adam**. For this optimizer,\n",
        "   a smaller learning rate usually works better, so the default learning\n",
        "   rate is set to 0.001.\n",
        "2. Since we get weight decay for free, you are welcome to use weight decay.\n",
        "\n",
        "Complete the function `run_pytorch_gradient_descent`, and use it to train\n",
        "your PyTorch MLP model to obtain a training accuracy of at least 38%.\n",
        "Plot the learning curve using the `plot_learning_curve` function provided\n",
        "to you, and include your plot in your PDF submission."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hRZG3o5FsDEA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def estimate_accuracy_torch(model, data, batch_size=5000, max_N=100000):\n",
        "    \"\"\"\n",
        "    Estimate the accuracy of the model on the data. To reduce\n",
        "    computation time, use at most `max_N` elements of `data` to\n",
        "    produce the estimate.\n",
        "    \"\"\"\n",
        "    correct = 0\n",
        "    N = 0\n",
        "    for i in range(0, data.shape[0], batch_size):\n",
        "        # get a batch of data\n",
        "        xs, ts = get_batch(data, i, i + batch_size, onehot=False)\n",
        "        \n",
        "        # forward pass prediction\n",
        "        y = model(torch.Tensor(xs))\n",
        "        y = y.detach().numpy() # convert the PyTorch tensor => numpy array\n",
        "        pred = np.argmax(y, axis=1)\n",
        "        correct += np.sum(pred == ts)\n",
        "        N += ts.shape[0]\n",
        "\n",
        "        if N > max_N:\n",
        "            break\n",
        "    return correct / N\n",
        "\n",
        "def run_pytorch_gradient_descent(model,\n",
        "                                 train_data=train4grams,\n",
        "                                 validation_data=valid4grams,\n",
        "                                 batch_size=100,\n",
        "                                 learning_rate=0.001,\n",
        "                                 weight_decay=0,\n",
        "                                 max_iters=1000,\n",
        "                                 checkpoint_path=None):\n",
        "    \"\"\"\n",
        "    Train the PyTorch model on the dataset `train_data`, reporting\n",
        "    the validation accuracy on `validation_data`, for `max_iters`\n",
        "    iteration.\n",
        "\n",
        "    If you want to **checkpoint** your model weights (i.e. save the\n",
        "    model weights to Google Drive), then the parameter\n",
        "    `checkpoint_path` should be a string path with `{}` to be replaced\n",
        "    by the iteration count:\n",
        "\n",
        "    For example, calling \n",
        "\n",
        "    >>> run_pytorch_gradient_descent(model, ...,\n",
        "            checkpoint_path = '/content/gdrive/My Drive/CSC321/mlp/ckpt-{}.pk')\n",
        "\n",
        "    will save the model parameters in Google Drive every 500 iterations.\n",
        "    You will have to make sure that the path exists (i.e. you'll need to create\n",
        "    the folder CSC321, mlp, etc...). Your Google Drive will be populated with files:\n",
        "\n",
        "    - /content/gdrive/My Drive/CSC321/mlp/ckpt-500.pk\n",
        "    - /content/gdrive/My Drive/CSC321/mlp/ckpt-1000.pk\n",
        "    - ...\n",
        "\n",
        "    To load the weights at a later time, you can run:\n",
        "\n",
        "    >>> model.load_state_dict(torch.load('/content/gdrive/My Drive/CSC321/mlp/ckpt-500.pk'))\n",
        "\n",
        "    This function returns the training loss, and the training/validation accuracy,\n",
        "    which we can use to plot the learning curve.\n",
        "    \"\"\"\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(),\n",
        "                           lr=learning_rate,\n",
        "                           weight_decay=weight_decay)\n",
        "\n",
        "    iters, losses = [], []\n",
        "    iters_sub, train_accs, val_accs  = [], [] ,[]\n",
        "\n",
        "    n = 0 # the number of iterations\n",
        "    while True:\n",
        "        for i in range(0, train_data.shape[0], batch_size):\n",
        "            if (i + batch_size) > train_data.shape[0]:\n",
        "                break\n",
        "\n",
        "            # get the input and targets of a minibatch\n",
        "            xs, ts = get_batch(train_data, i, i + batch_size, onehot=False)\n",
        "\n",
        "            # convert from numpy arrays to PyTorch tensors\n",
        "            xs = torch.Tensor(xs)\n",
        "            ts = torch.Tensor(ts).long()\n",
        "\n",
        "            zs = model(xs)                 # compute prediction logit\n",
        "            loss = criterion(zs, ts)                  # compute the total loss\n",
        "            loss.backward()                      # compute updates for each parameter\n",
        "            optimizer.step()                      # make the updates for each parameter\n",
        "            optimizer.zero_grad()                      # a clean up step for PyTorch\n",
        "\n",
        "            # save the current training information\n",
        "            iters.append(n)\n",
        "            losses.append(float(loss)/batch_size)  # compute *average* loss\n",
        "\n",
        "            if n % 500 == 0:\n",
        "                iters_sub.append(n)\n",
        "                train_cost = float(loss.detach().numpy())\n",
        "                train_acc = estimate_accuracy_torch(model, train_data, batch_size=10, max_N=100)\n",
        "                train_accs.append(train_acc)\n",
        "                val_acc = estimate_accuracy_torch(model, validation_data, batch_size=10, max_N=100)\n",
        "                val_accs.append(val_acc)\n",
        "                print(\"Iter %d. [Val Acc %.0f%%] [Train Acc %.0f%%, Loss %f]\" % (\n",
        "                      n, val_acc * 100, train_acc * 100, train_cost))\n",
        "\n",
        "                if (checkpoint_path is not None) and n > 0:\n",
        "                    torch.save(model.state_dict(), checkpoint_path.format(n))\n",
        "\n",
        "            # increment the iteration number\n",
        "            n += 1\n",
        "\n",
        "            if n > max_iters:\n",
        "                return iters, losses, iters_sub, train_accs, val_accs\n",
        "\n",
        "\n",
        "def plot_learning_curve(iters, losses, iters_sub, train_accs, val_accs):\n",
        "    \"\"\"\n",
        "    Plot the learning curve.\n",
        "    \"\"\"\n",
        "    plt.title(\"Learning Curve: Loss per Iteration\")\n",
        "    plt.plot(iters, losses, label=\"Train\")\n",
        "    plt.xlabel(\"Iterations\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.show()\n",
        "\n",
        "    plt.title(\"Learning Curve: Accuracy per Iteration\")\n",
        "    plt.plot(iters_sub, train_accs, label=\"Train\")\n",
        "    plt.plot(iters_sub, val_accs, label=\"Validation\")\n",
        "    plt.xlabel(\"Iterations\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.legend(loc='best')\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zDAY3u2SsDEG",
        "colab_type": "code",
        "outputId": "0b2a6f49-7082-4169-ec92-9c1f516c9520",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 775
        }
      },
      "source": [
        "pytorch_mlp = PyTorchMLP()\n",
        "learning_curve_info = run_pytorch_gradient_descent(pytorch_mlp, batch_size=200, weight_decay=0, max_iters=5000, learning_rate=0.001, checkpoint_path='/content/gdrive/My Drive/CSC321/mlp/ckpt-{}.pk')\n",
        "\n",
        "# you might want to save the `learning_curve_info` somewhere, so that you can plot\n",
        "# the learning curve prior to exporting your PDF file\n",
        "\n",
        "plot_learning_curve(*learning_curve_info)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iter 0. [Val Acc 2%] [Train Acc 5%, Loss 5.519195]\n",
            "Iter 500. [Val Acc 27%] [Train Acc 26%, Loss 2.964610]\n",
            "Iter 1000. [Val Acc 31%] [Train Acc 28%, Loss 2.780205]\n",
            "Iter 1500. [Val Acc 30%] [Train Acc 33%, Loss 2.705700]\n",
            "Iter 2000. [Val Acc 31%] [Train Acc 33%, Loss 2.576844]\n",
            "Iter 2500. [Val Acc 35%] [Train Acc 32%, Loss 2.764725]\n",
            "Iter 3000. [Val Acc 33%] [Train Acc 31%, Loss 2.754182]\n",
            "Iter 3500. [Val Acc 34%] [Train Acc 28%, Loss 2.680690]\n",
            "Iter 4000. [Val Acc 32%] [Train Acc 31%, Loss 2.724346]\n",
            "Iter 4500. [Val Acc 34%] [Train Acc 31%, Loss 2.739720]\n",
            "Iter 5000. [Val Acc 36%] [Train Acc 35%, Loss 2.390934]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZUAAAEWCAYAAACufwpNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngooxxpiEsaBijDEmYSyoGGOMSZj/B3+fXdC1G/h4AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jt2IdsLUsDEL",
        "colab_type": "text"
      },
      "source": [
        "### Part (e) -- 3 points\n",
        "\n",
        "Write a function `make_prediction` that takes as parameters\n",
        "a PyTorchMLP model and sentence (a list of words), and produces\n",
        "a prediction for the next word in the sentence.\n",
        "\n",
        "Start by thinking about what you need to do, step by step, taking\n",
        "care of the difference between a numpy array and a PyTorch Tensor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZSnG4DcsDEP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_prediction_torch(model, sentence):\n",
        "    \"\"\"\n",
        "    Use the model to make a prediction for the next word in the\n",
        "    sentence using the last 3 words (sentence[:-3]). You may assume\n",
        "    that len(sentence) >= 3 and that `model` is an instance of\n",
        "    PYTorchMLP.\n",
        "\n",
        "    This function should return the next word, represented as a string.\n",
        "\n",
        "    Example call:\n",
        "    >>> make_prediction_torch(pytorch_mlp, ['you', 'are', 'a'])\n",
        "    \"\"\"\n",
        "    global vocab_stoi, vocab_itos\n",
        "\n",
        "    #  Write your code here\n",
        "    indices = []\n",
        "    indices.append(vocab_stoi[sentence[-3]])\n",
        "    indices.append(vocab_stoi[sentence[-2]])\n",
        "    indices.append(vocab_stoi[sentence[-1]])\n",
        "    data = torch.tensor(make_onehot(indices).reshape([1, -1, 250])).float()\n",
        "    out = model(data).data.numpy()\n",
        "    word = np.argmax(out)\n",
        "    return vocab_itos[word]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3PK_zPbTsDEV",
        "colab_type": "text"
      },
      "source": [
        "### Part (f) -- 4 points\n",
        "\n",
        "Use your code to predict what the next word should be in each\n",
        "of the following sentences:\n",
        "\n",
        "- \"You are a\"\n",
        "- \"few companies show\"\n",
        "- \"There are no\"\n",
        "- \"yesterday i was\"\n",
        "- \"the game had\"\n",
        "- \"yesterday the federal\"\n",
        "\n",
        "Do your predictions make sense? (If all of your predictions are the same,\n",
        "train your model for more iterations, or change the hyperparameters in your\n",
        "model. You may need to do this even if your training accuracy is >=38%)\n",
        "\n",
        "One concern you might have is that our model may be \"memorizing\" information\n",
        "from the training set.  Check if each of 3-grams (the 3 words appearing next\n",
        "to each other) appear in the training set. If so, what word occurs immediately\n",
        "following those three words?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r9whrKKYsDEX",
        "colab_type": "code",
        "outputId": "d107701d-53f9-4020-a361-a9fbdee9d05a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "source": [
        "# Write your code and answers here\n",
        "print(make_prediction_torch(pytorch_mlp, ['you','are','a']))\n",
        "print(make_prediction_torch(pytorch_mlp, ['few','companies','show']))\n",
        "print(make_prediction_torch(pytorch_mlp, ['there','are','no']))\n",
        "print(make_prediction_torch(pytorch_mlp, ['yesterday','i','was']))\n",
        "print(make_prediction_torch(pytorch_mlp, ['the','game','had']))\n",
        "print(make_prediction_torch(pytorch_mlp, ['yesterday','the','federal']))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "good\n",
            ".\n",
            "other\n",
            "nt\n",
            "to\n",
            "government\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1ttJ0xNsDEd",
        "colab_type": "text"
      },
      "source": [
        "### Part (g) -- 1 points\n",
        "\n",
        "Report the test accuracy of your model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "95juKs_NsDEf",
        "colab_type": "code",
        "outputId": "b29631d8-c0ac-4183-823a-40659cbe0b61",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Write your code here\n",
        "test_acc = estimate_accuracy_torch(pytorch_mlp, test4grams)\n",
        "print(\"Test accuracy = \"+str(test_acc))\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test accuracy = 0.3669136833315938\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3tbCLmPsDEl",
        "colab_type": "text"
      },
      "source": [
        "## Question 3. Learning Word Embeddings\n",
        "\n",
        "In this section, we will build a slightly different model with a different\n",
        "architecture. In particular, we will first compute a lower-dimensional\n",
        "*representation* of the three words, before using a multi-layer perceptron.\n",
        "\n",
        "Our model will look like this:\n",
        "\n",
        "<img src=\"https://www.cs.toronto.edu/~lczhang/321/hw/p2_model2.png\" />\n",
        "\n",
        "This model has 3 layers instead of 2, but the first layer of the network\n",
        "is **not** fully-connected. Instead, we compute the representations of each\n",
        "of the three words **separately**. In addition, the first layer of the network\n",
        "will not use any biases. The reason for this will be clear in question 4.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_6jL5a__esa",
        "colab_type": "text"
      },
      "source": [
        "### Part (a) - 10 pts\n",
        "\n",
        "Complete the methods in `NumpyWordEmbModel`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OTDbVdQ48M6f",
        "colab_type": "code",
        "outputId": "a6a18b3a-ecbf-422d-a47d-dccf9cdd4db3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 274
        }
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[[ 0  1  2]\n",
            "  [ 3  4  5]\n",
            "  [ 6  7  8]]\n",
            "\n",
            " [[ 9 10 11]\n",
            "  [12 13 14]\n",
            "  [15 16 17]]\n",
            "\n",
            " [[18 19 20]\n",
            "  [21 22 23]\n",
            "  [24 25 26]]]\n",
            "[[ 0  1  2  3  4  5  6  7  8]\n",
            " [ 9 10 11 12 13 14 15 16 17]\n",
            " [18 19 20 21 22 23 24 25 26]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AeFZI6xQsDEm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NumpyWordEmbModel(object):\n",
        "    def __init__(self, vocab_size=250, emb_size=100, num_hidden=100):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.emb_size = emb_size\n",
        "        self.num_hidden = num_hidden\n",
        "        self.emb_weights = np.zeros([emb_size, vocab_size]) # no biases in this layer\n",
        "        self.weights1 = np.zeros([num_hidden, emb_size * 3])\n",
        "        self.bias1 = np.zeros([num_hidden])\n",
        "        self.weights2 = np.zeros([vocab_size, num_hidden])\n",
        "        self.bias2 = np.zeros([vocab_size])\n",
        "        self.cleanup()\n",
        "\n",
        "    def initializeParams(self):\n",
        "        \"\"\"\n",
        "        Randomly initialize the weights and biases of this two-layer MLP.\n",
        "        The randomization is necessary so that each weight is updated to\n",
        "        a different value.\n",
        "        \"\"\"\n",
        "        self.emb_weights = np.random.normal(0, 2/self.num_hidden, self.emb_weights.shape)\n",
        "        self.weights1 = np.random.normal(0, 2/self.num_features, self.weights1.shape)\n",
        "        self.bias1 = np.random.normal(0, 2/self.num_features, self.bias1.shape)\n",
        "        self.weights2 = np.random.normal(0, 2/self.num_hidden, self.weights2.shape)\n",
        "        self.bias2 = np.random.normal(0, 2/self.num_hidden, self.bias2.shape)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        \"\"\"\n",
        "        Compute the forward pass prediction for inputs.\n",
        "        Note that `inputs` will be a rank-3 numpy array with shape [N, 3, 250].\n",
        "\n",
        "        For numerical stability reasons, we **do not** apply the softmax\n",
        "        activation in the forward function. The loss function assumes that \n",
        "        we return the logits from this function.\n",
        "        \"\"\"\n",
        "\n",
        "        #Get activation values for the first hidden layer\n",
        "        #Get activation values for the output layer\n",
        "        #250 is the number of words\n",
        "\n",
        "        X = inputs.reshape([-1, 3*250])\n",
        "\n",
        "        z0 = np.zeros([self.emb_weights.shape[0], 1])\n",
        "        z1 = np.zeros([self.weights1.shape[0], 1])\n",
        "        z2 = np.zeros([self.weights2.shape[1], 1))\n",
        "\n",
        "\n",
        "        \n",
        "        # TODO\n",
        "\n",
        "    def __call__(self, inputs):\n",
        "        return self.forward(inputs)\n",
        "\n",
        "    def backward(self, ts):\n",
        "        \"\"\"\n",
        "        Compute the backward pass, given the ground-truth, one-hot targets.\n",
        "        Note that `ts` needs to be a rank 2 numpy array with shape [N, 250].\n",
        "\n",
        "        Remember the multivariate chain rule: if a weight affects the loss\n",
        "        through different paths, then the error signal from all the paths\n",
        "        must be added together.\n",
        "        \"\"\"\n",
        "        # TODO\n",
        "\n",
        "    def update(self, alpha):\n",
        "        \"\"\"\n",
        "        Compute the gradient descent update for the parameters.\n",
        "        \"\"\"\n",
        "        # TODO\n",
        "\n",
        "    def cleanup(self):\n",
        "        \"\"\"\n",
        "        Erase the values of the variables that we use in our computation.\n",
        "        \"\"\"\n",
        "        # TODO"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHgyAipUsDEr",
        "colab_type": "text"
      },
      "source": [
        "### Part (b) -- 1 pts\n",
        "\n",
        "One strategy that machine learning practitioners use to debug their code\n",
        "is to *first try to overfit their model to a small training set*. If the\n",
        "gradient computation is correct and the data is encoded properly, then your\n",
        "model should easily achieve 100% training accuracy on a small training set.\n",
        "\n",
        "Show that your model is implemented correctly by showing that your model\n",
        "can achieve an 100% training accuracy within a few hundred iterations, when\n",
        "using a small training set (e.g. one batch)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKRoMCVXsDEr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "numpy_wordemb = NumpyWordEmbModel()\n",
        "run_pytorch_gradient_descent(numpy_wordemb, train4grams[:64], batch_size=64, ...)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mcW9jxbksDEy",
        "colab_type": "text"
      },
      "source": [
        "### Part (c) -- 2 pts\n",
        "\n",
        "Train your model from part (a) to obtain a training accuracy of at least 25%."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-OXmyn_sDE1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Your code goes here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8lbNtm_LsDE-",
        "colab_type": "text"
      },
      "source": [
        "### Part (d) -- 2 pts\n",
        "\n",
        "The PyTorch version of the model is implemented for you. Use \n",
        "`run_pytorch_gradient_descent` to train\n",
        "your PyTorch MLP model to obtain a training accuracy of at least 38%.\n",
        "Plot the learning curve using the `plot_learning_curve` function provided\n",
        "to you, and include your plot in your PDF submission.\n",
        "\n",
        "Make sure that you checkpoint frequently. We will be using ..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HmO1qDXMsDFB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PyTorchWordEmb(nn.Module):\n",
        "    def __init__(self, emb_size=100, num_hidden=300, vocab_size=250):\n",
        "        super(PyTorchWordEmb, self).__init__()\n",
        "        self.word_emb_layer = nn.Linear(vocab_size, emb_size, bias=False)\n",
        "        self.fc_layer1 = nn.Linear(emb_size * 3, num_hidden)\n",
        "        self.fc_layer2 = nn.Linear(num_hidden, 250)\n",
        "        self.num_hidden = num_hidden\n",
        "        self.emb_size = emb_size\n",
        "    def forward(self, inp):\n",
        "        embeddings = torch.relu(self.word_emb_layer(inp))\n",
        "        embeddings = embeddings.reshape([-1, self.emb_size * 3])\n",
        "        hidden = torch.relu(self.fc_layer1(embeddings))\n",
        "        return self.fc_layer2(hidden)\n",
        "\n",
        "# pytorch_wordemb= PyTorchWordEmb()\n",
        "\n",
        "# result = run_pytorch_gradient_descent(pytorch_wordemb,\n",
        "#                                       max_iters=20000,\n",
        "#                                       ...)\n",
        "\n",
        "# plot_learning_curve(*result)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2MO4VHzxsDFF",
        "colab_type": "text"
      },
      "source": [
        "### Part (e) -- 2 pts\n",
        "\n",
        "Use the function `make_prediction` that you wrote earlier to  \n",
        "predict what the next word should be in each of the following sentences:\n",
        "\n",
        "- \"You are a\"\n",
        "- \"few companies show\"\n",
        "- \"There are no\"\n",
        "- \"yesterday i was\"\n",
        "- \"the game had\"\n",
        "- \"yesterday the federal\"\n",
        "\n",
        "How do these predictions compared to the previous model?\n",
        "\n",
        "Just like before, if all of your predictions are the same,\n",
        "train your model for more iterations, or change the hyperparameters in your\n",
        "model. You may need to do this even if your training accuracy is >=38%."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-aHJ62TsDFF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Your code goes here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77wk9K6nsDFL",
        "colab_type": "text"
      },
      "source": [
        "### Part (f) -- 1 pts\n",
        "\n",
        "Report the test accuracy of your model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TvW2hMPVsDFM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Write your code here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VD9LUFVwsDFS",
        "colab_type": "text"
      },
      "source": [
        "## Question 4. Visualizing Word Embeddings\n",
        "\n",
        "While training the `PyTorchMLP`, we trained the `word_emb_layer`, which takes a one-hot\n",
        "representation of a word in our vocabulary, and returns a low-dimensional vector\n",
        "representation of that word. In this question, we will explore these word embeddings.\n",
        "\n",
        "### Part (a) -- 2 pts\n",
        "\n",
        "The code below extracts the **weights** of the word embedding layer,\n",
        "and converts the PyTorch tensor into an numpy array.\n",
        "Explain why each *row* of `word_emb` contains the vector representing\n",
        "of a word. For example `word_emb[vocab_stoi[\"any\"],:]` contains the\n",
        "vector representation of the word \"any\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32WiI0H3sDFT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_emb_weights = list(pytorch_wordemb.word_emb_layer.parameters())[0]\n",
        "word_emb = word_emb_weights.detach().numpy().T\n",
        "\n",
        "# Write your explanation here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IuZ5rMG_sDFY",
        "colab_type": "text"
      },
      "source": [
        "### Part (b) -- 2 pts\n",
        "\n",
        "Once interesting thing about these word embeddings is that distances\n",
        "in these vector representations of words make some sense! To show this,\n",
        "we have provided code below that computes the cosine similarity of\n",
        "every pair of words in our vocabulary. This code should look familiar,\n",
        "since we have seen it in project 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7OJL-T7sDFZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "norms = np.linalg.norm(word_emb, axis=1)\n",
        "word_emb_norm = (word_emb.T / norms).T\n",
        "similarities = np.matmul(word_emb_norm, word_emb_norm.T)\n",
        "\n",
        "# Some example distances. The first one should be larger than the second\n",
        "print(similarities[vocab_stoi['any'], vocab_stoi['many']])\n",
        "print(similarities[vocab_stoi['any'], vocab_stoi['government']])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yzexj3E5sDFe",
        "colab_type": "text"
      },
      "source": [
        "Compute the 5 closest words to the following words:\n",
        "\n",
        "- \"four\"\n",
        "- \"go\"\n",
        "- \"what\"\n",
        "- \"should\"\n",
        "- \"school\"\n",
        "- \"your\"\n",
        "- \"yesterday\"\n",
        "- \"not\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uaZnEaipsDFg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Write your code here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2Eicd9QsDFm",
        "colab_type": "text"
      },
      "source": [
        "### Part (c) -- 2 pts\n",
        "\n",
        "We can visualize the word embeddings by reducing the dimensionality of\n",
        "the word vectors to 2D. There are many dimensionality reduction techniques\n",
        "that we could use, and we will use an algorithm called t-SNE.\n",
        "(You don’t need to know what this is for the assignment,\n",
        "but we may cover it later in the course.)\n",
        "Nearby points in this 2-D space are meant to correspond to nearby points\n",
        "in the original, high-dimensional space.\n",
        "\n",
        "The following code runs the t-SNE algorithm and plots the result.\n",
        "Look at the plot and find two clusters of related words.\n",
        "What do the words in each cluster have in common?\n",
        "\n",
        "Note that there is randomness in the initialization of the t-SNE \n",
        "algorithm. If you re-run this code, you may get a different image.\n",
        "Please make sure to submit your image in the PDF file for your TA to see."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pVBmQpuJsDFm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sklearn.manifold\n",
        "tsne = sklearn.manifold.TSNE()\n",
        "Y = tsne.fit_transform(word_emb)\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.xlim(Y[:,0].min(), Y[:, 0].max())\n",
        "plt.ylim(Y[:,1].min(), Y[:, 1].max())\n",
        "for i, w in enumerate(vocab):\n",
        "    plt.text(Y[i, 0], Y[i, 1], w)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}